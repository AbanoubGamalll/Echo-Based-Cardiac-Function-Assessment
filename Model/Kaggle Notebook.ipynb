{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7548742,"sourceType":"datasetVersion","datasetId":4396335},{"sourceId":166316293,"sourceType":"kernelVersion"},{"sourceId":174218815,"sourceType":"kernelVersion"},{"sourceId":40984,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":34487},{"sourceId":40993,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":34495},{"sourceType":"modelInstanceVersion","sourceId":42388,"isSourceIdPinned":true}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"papermill":{"default_parameters":{},"duration":2469.637109,"end_time":"2024-04-28T19:26:26.199123","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-28T18:45:16.562014","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<pre>\nSteps To Run the Notebook in Kaggle\n- To Train Transformer Model use \"GPU T4 x2\" Accelerator \n- To Train ED & ES Model use \"TPU VM v3-8\" Accelerator \n- To Test Models use \"TPU VM v3-8\" Accelerator\n\n** Add Input **\n    1) Add DATASETS \n        - EchoNet-Dynamic >> https://www.kaggle.com/datasets/mahnurrahman/echonet-dynamic\n    2) Add MODELS\n        - Models(Transformer) >> https://www.kaggle.com/models/abanoubgamal/u-net/Keras/transformer/1\n        - Models(ED) >> https://www.kaggle.com/models/abanoubgamal/u-net/Keras/ed/1\n        - Models(ES) >> https://www.kaggle.com/models/abanoubgamal/u-net/Keras/es/1\n    3) Add NOTEBOOKS\n        - U-NET-Frames-Masks >> https://www.kaggle.com/code/abanoubgamal/u-net-frames-masks\n        - LoadedVideos >> https://www.kaggle.com/code/abanoubgamal/loadedvideos\n</pre>\nNow You Are Ready to run","metadata":{}},{"cell_type":"markdown","source":"# Import Library","metadata":{"papermill":{"duration":0.005667,"end_time":"2024-04-28T18:45:18.335747","exception":false,"start_time":"2024-04-28T18:45:18.330080","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Run this line when use \"TPU VM v3-8\" Accelerator only\n# pip install --upgrade tensorflow","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:45:18.349053Z","iopub.status.busy":"2024-04-28T18:45:18.348729Z","iopub.status.idle":"2024-04-28T18:46:24.492696Z","shell.execute_reply":"2024-04-28T18:46:24.491875Z"},"papermill":{"duration":66.152862,"end_time":"2024-04-28T18:46:24.494580","exception":false,"start_time":"2024-04-28T18:45:18.341718","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport cv2\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport imageio.v2 as imageio\nimport matplotlib.pyplot as plt\nfrom transformers import BertConfig,BertModel\nimport torch\nimport torch.nn as nn\nimport tqdm\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nfrom keras.src.layers import concatenate, Dropout, Conv2DTranspose, Input, Conv2D, MaxPooling2D\nfrom keras.src.losses import SparseCategoricalCrossentropy","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:46:24.546077Z","iopub.status.busy":"2024-04-28T18:46:24.545727Z","iopub.status.idle":"2024-04-28T18:47:10.366863Z","shell.execute_reply":"2024-04-28T18:47:10.365805Z"},"papermill":{"duration":45.849747,"end_time":"2024-04-28T18:47:10.369411","exception":false,"start_time":"2024-04-28T18:46:24.519664","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Paths","metadata":{"papermill":{"duration":0.023487,"end_time":"2024-04-28T18:47:10.417130","exception":false,"start_time":"2024-04-28T18:47:10.393643","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# DataSet\n\"\"\" Dataset Files\nEchoNet-Dynamic Dataset\n├── FileList.csv\n├── VolumeTracings.csv\n└── Videos\n    ├── 0X1A0A263B22CCD966.avi\n    ├── 0X1A2A76BDB5B98BED.avi\n    ├── 0X1A2C60147AF9FDAE.avi\n    └── etc.\n\"\"\"\n_dataRootPath = '/kaggle/input/echonet-dynamic/EchoNet-Dynamic'\n_videosPath = _dataRootPath + '/Videos'\n_fileNamesPath = _dataRootPath + '/FileList.csv'\n_volumeTracingPath = _dataRootPath + '/VolumeTracings.csv'\n\n# Loaded Videos\n_loadedVideosPath = '/kaggle/input/loadedvideos'\n\n# Transformer\n_transformerModelPath = '/kaggle/input/transformermodel/best.pt'\n\n# U-NET\n_trueMasksPath = '/kaggle/input/u-net-frames-masks'\n\n_ED_Model_Path = '/kaggle/input/u-net/keras/ed/1/ED_U_NET_Model.weights.h5'\n_ES_Model_Path = '/kaggle/input/u-net/keras/es/1/ES_Best.weights.h5'","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:10.466236Z","iopub.status.busy":"2024-04-28T18:47:10.465721Z","iopub.status.idle":"2024-04-28T18:47:10.470581Z","shell.execute_reply":"2024-04-28T18:47:10.469870Z"},"papermill":{"duration":0.031352,"end_time":"2024-04-28T18:47:10.472232","exception":false,"start_time":"2024-04-28T18:47:10.440880","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# DataModel","metadata":{"papermill":{"duration":0.023824,"end_time":"2024-04-28T18:47:10.519351","exception":false,"start_time":"2024-04-28T18:47:10.495527","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class LandMarks:\n    def __init__(self, X1, Y1, X2, Y2):\n        self.X1 = X1\n        self.Y1 = Y1\n        self.X2 = X2\n        self.Y2 = Y2\n\n    def displayInfo(self):\n        print(f\"\"\"\n              land Marks are :\n                    X1 is  {self.X1}\n                    Y1 is {self.Y1}\n                    X2 is {self.X2}\n                    Y2 is {self.Y2}\"\"\")\n\n\nclass VideoData:\n    def __init__(self, fileName, EF_value, ED_value, ES_value, ED_frame, ES_frame, Split, ED_landMark, ES_landMark,\n                 numberOfFrames,\n                 ED_Frame_IMG, ES_Frame_IMG):\n        self.fileName = fileName\n        self.EF_value = EF_value\n        self.ED_value = ED_value\n        self.ES_value = ES_value\n        self.ED_frame = ED_frame\n        self.ES_frame = ES_frame\n        self.Split = Split\n        self.ED_landMark = ED_landMark\n        self.ES_landMark = ES_landMark\n        self.numberOfFrames = numberOfFrames\n        self.ED_Frame_IMG = ED_Frame_IMG\n        self.ES_Frame_IMG = ES_Frame_IMG\n\n    def displayInfo(self):\n        print(f\"\"\"\n        Video Information:\n              File Name is  {self.fileName}\n              EF Value is {self.EF_value}\n              ES Value is {self.ES_value}\n              ED Value is {self.ED_value}\n              ED Frame is {self.ED_frame}\n              ES Frame is {self.ES_frame}\n              Split is {self.Split}\n              numberOfFrames is {self.numberOfFrames}\"\"\")","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:10.568074Z","iopub.status.busy":"2024-04-28T18:47:10.567801Z","iopub.status.idle":"2024-04-28T18:47:10.574773Z","shell.execute_reply":"2024-04-28T18:47:10.574161Z"},"papermill":{"duration":0.033529,"end_time":"2024-04-28T18:47:10.576397","exception":false,"start_time":"2024-04-28T18:47:10.542868","status":"completed"},"tags":[]},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# HelperFunction","metadata":{"papermill":{"duration":0.023548,"end_time":"2024-04-28T18:47:10.623555","exception":false,"start_time":"2024-04-28T18:47:10.600007","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def _FilterNot_42rows(VolumeTracings, FileList):\n    VolumeTracings.dropna(inplace=True)\n    FileList.dropna(inplace=True)\n    VolumeTracings_names = VolumeTracings['FileName']\n    # VolumeTracings_names_no_extension = np.array([name[:-4] for name in VolumeTracings_names])\n\n    VolumeUniqueName, frame_counter = np.unique(VolumeTracings_names, return_counts=True)\n    Video_counts = dict(zip(VolumeUniqueName, frame_counter))\n\n    no_rows = 0\n    not_42_Rows_video_names = []\n    for vName, count in Video_counts.items():\n        if count != 42:\n            no_rows += count\n            not_42_Rows_video_names.append(vName)\n\n    VolumeTracings = VolumeTracings[~VolumeTracings['FileName'].isin(not_42_Rows_video_names)]\n    FileList = FileList[(FileList['FileName'] + \".avi\").isin(VolumeTracings['FileName'])]\n\n    # Delete rows where 'FileName' column has value '0X4F8859C8AB4DA9CB.avi'\n    VolumeTracings = VolumeTracings[VolumeTracings['FileName'] != '0X4F8859C8AB4DA9CB.avi']\n\n    return VolumeTracings, FileList\n\n\ndef _loadAlldata(split_type):\n    FileList = pd.read_csv(_fileNamesPath)\n    VolumeTracings = pd.read_csv(_volumeTracingPath)\n\n    VolumeTracings, FileList = _FilterNot_42rows(VolumeTracings, FileList)\n\n    leftVentricle_list = []\n\n    VolumeTracings.dropna(inplace=True)\n    FileList.dropna(inplace=True)\n\n    for i in range(FileList.iloc[:, 0].size):\n\n        Split = FileList.iloc[i, 8]\n        if split_type != \"ALL\":\n            if split_type != Split:\n                continue\n        fileName = FileList.iloc[i, 0]\n\n        VT = VolumeTracings[VolumeTracings['FileName'] == fileName + '.avi']\n        unique_Frames = VT['Frame'].unique()\n\n        if len(unique_Frames) == 0:\n            continue\n\n        ED_Frame = unique_Frames[0]\n\n        ES_Frame = unique_Frames[1]\n        ED_tmp = VT[VT['Frame'] == ED_Frame]\n        ES_tmp = VT[VT['Frame'] == ES_Frame]\n\n        if len(ED_tmp) != 21 or len(ES_tmp) != 21:\n            continue\n        ED_landMark = LandMarks([], [], [], [])\n        ES_landMark = LandMarks([], [], [], [])\n\n        for k in range(21):\n            ED_landMark.X1.append(ED_tmp.iloc[k, 1])\n            ED_landMark.Y1.append(ED_tmp.iloc[k, 2])\n            ED_landMark.X2.append(ED_tmp.iloc[k, 3])\n            ED_landMark.Y2.append(ED_tmp.iloc[k, 4])\n\n            ES_landMark.X1.append(ES_tmp.iloc[k, 1])\n            ES_landMark.Y1.append(ES_tmp.iloc[k, 2])\n            ES_landMark.X2.append(ES_tmp.iloc[k, 3])\n            ES_landMark.Y2.append(ES_tmp.iloc[k, 4])\n\n        EF_value = FileList.iloc[i, 1]\n        ED_value = FileList.iloc[i, 2]\n        ES_value = FileList.iloc[i, 3]\n        numberOfFrames = FileList.iloc[i, 7]\n\n        video_path = os.path.join(_videosPath, fileName + '.avi')\n\n        cap = cv2.VideoCapture(video_path)\n\n        if not cap.isOpened():\n            print(\"Error opening video file\")\n\n        cap.set(cv2.CAP_PROP_POS_FRAMES, ED_Frame - 1)\n        _, ED_Frame_IMG = cap.read()\n\n        cap.set(cv2.CAP_PROP_POS_FRAMES, ES_Frame - 1)\n        _, ES_Frame_IMG = cap.read()\n\n        cap.release()\n\n        obj = VideoData(fileName, EF_value, ED_value, ES_value, ED_Frame, ES_Frame, Split, ED_landMark,\n                        ES_landMark, numberOfFrames, ED_Frame_IMG, ES_Frame_IMG)\n\n        leftVentricle_list.append(obj)\n    return leftVentricle_list\n\n\ndef load_or_get_data(spilt_type=\"ALL\"):\n    if spilt_type not in ['TRAIN', 'TEST', 'VAL', 'ALL']:\n        print('Error not valid split type')\n        return None\n\n    if not os.path.exists(_loadedVideosPath):\n        os.makedirs(_loadedVideosPath)\n        print(f'{_loadedVideosPath} created')\n\n    file_path = f'{_loadedVideosPath}/Loaded_Videos_Objects_{spilt_type}.pkl'\n    # If file exists, load the data from the file\n    if os.path.exists(file_path):\n        with open(file_path, 'rb') as f:\n            data = pickle.load(f)\n    # If file doesn't exist, execute loadAlldata() to get the data\n    else:\n        data = _loadAlldata(spilt_type)\n        with open(file_path, 'wb') as f:\n            pickle.dump(data, f)\n\n    return data\n\n\ndef _extractVideoFrames(path):\n    capture = cv2.VideoCapture(str(path))\n\n    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frames = np.zeros((frame_count, frame_width, frame_height, 3), np.uint8)\n\n    for count in range(frame_count):\n        ret, frame = capture.read()\n        if not ret:\n            raise ValueError(\"Failed to load frame #{} of {}.\".format(count, path))\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frames[count] = frame\n\n    return frames\n\n\n# Transformer Data\ndef _mirroringVideo(video_obj):\n    original_tuple = []\n    desired_length = 128\n\n    path = os.path.join(_videosPath, video_obj.fileName + '.avi')\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(path)\n\n    v = _extractVideoFrames(path)\n\n    # Mirror\n    start = min(video_obj.ED_frame, video_obj.ES_frame)\n    end = max(video_obj.ED_frame, video_obj.ES_frame) + 1\n\n    for i in range(start, end):\n        img = v[i]\n        if video_obj.ED_frame == i:\n            original_tuple.append((img, \"ED\"))\n        elif video_obj.ES_frame == i:\n            original_tuple.append((img, \"ES\"))\n        else:\n            original_tuple.append((img, \"Transition\"))\n\n    while len(original_tuple) < desired_length:\n\n        # Create a mirrored dictionary by reversing keys and values\n        mirrored_tuple = list(reversed(original_tuple))[1:]\n\n        # Append the mirrored list\n        original_tuple.extend(mirrored_tuple)\n\n        # If the list length exceeds desired_length, break the loop\n        if len(original_tuple) >= desired_length:\n            break\n\n        # Append the original list again\n        original_tuple.extend(original_tuple[1:])\n\n    # Trim the dictionary to desired_length if it exceeds it\n    original_tuple = original_tuple[:desired_length]\n\n    return original_tuple\n\n\n# U-Net Data\ndef _prepareDataToPolygon(landmark):\n    data = []\n    for i in range(21):\n        data.append((landmark.X1[i], landmark.Y1[i]))\n\n    for i in range(21):\n        data.append((landmark.X2[i], landmark.Y2[i]))\n\n    if data[0][1] > data[21][1]:\n        tmp = data[0]\n        data[0] = data[21]\n        data[21] = tmp\n\n    if data[21][0] < data[20][0]:\n        tmp = data[21]\n        data[21] = data[20]\n        data[20] = tmp\n\n    tmp = data[22:]\n    data[22:] = tmp[::-1]\n\n    return data\n\n\ndef _createBinaryMask(landmark):\n    vertices = _prepareDataToPolygon(landmark)\n\n    # Create an empty black image\n    mask = np.zeros((112, 112)).astype(float)\n\n    vertices = np.array(vertices)\n    vertices = np.round(vertices)\n    pts = vertices.astype(int)\n\n    cv2.fillPoly(mask, [pts], color=(255, 255, 255))\n\n    mask[mask == 255] = 1\n\n    return mask\n\n\ndef _createImageAndMaskFolders(frameType, split, path):\n    image_path = path + f'/Frames_{frameType}/'\n    mask_path = path + f'/Masks_{frameType}/'\n\n    try:\n        os.makedirs(image_path)\n        print(f'Frames_{frameType} create')\n    except OSError:\n        print(f'Frames_{frameType} is exist')\n\n    try:\n        os.makedirs(mask_path)\n        print(f'Masks_{frameType} create')\n    except OSError:\n        print(f'Masks_{frameType} is exist')\n\n    image_path += f'{split}/'\n    mask_path += f'{split}/'\n\n    try:\n        os.makedirs(image_path)\n        print(f'Frames_{frameType}/{split} create')\n\n    except OSError:\n        print(f'Frames_{frameType}/{split} is exist')\n\n    try:\n        os.makedirs(mask_path)\n        print(f'Masks_{frameType}/{split} create')\n\n    except OSError:\n        print(f'Masks_{frameType}/{split} is exist')\n    return image_path, mask_path\n\n\ndef _saveImageAndMask(frameType, split, trueMasksPath='', ):\n    data_set = load_or_get_data(split)\n    if data_set is None:\n        return\n\n    image_path, mask_path = _createImageAndMaskFolders(frameType, split, trueMasksPath)\n    img = None\n    landmarks = None\n    for obj in data_set:\n        if frameType == 'ES':\n            img = obj.ES_Frame_IMG\n            landmarks = obj.ES_landMark\n        elif frameType == 'ED':\n            img = obj.ED_Frame_IMG\n            landmarks = obj.ED_landMark\n\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        mask = _createBinaryMask(landmarks)\n\n        cv2.imwrite(mask_path + f'{obj.fileName}.png', mask)\n        cv2.imwrite(image_path + f'{obj.fileName}.png', img)\n\n\ndef CreateAllMasks(trueMasksPath):\n    _saveImageAndMask(frameType='ES', split='TRAIN', trueMasksPath=trueMasksPath)\n    _saveImageAndMask(frameType='ES', split='TEST', trueMasksPath=trueMasksPath)\n    _saveImageAndMask(frameType='ES', split='VAL', trueMasksPath=trueMasksPath)\n\n    _saveImageAndMask(frameType='ED', split='TRAIN', trueMasksPath=trueMasksPath)\n    _saveImageAndMask(frameType='ED', split='TEST', trueMasksPath=trueMasksPath)\n    _saveImageAndMask(frameType='ED', split='VAL', trueMasksPath=trueMasksPath)\n\n\n# Read Data\ndef _process_path(image_path, mask_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n    return img, mask\n\n\ndef getImageAndMasks(frameType='', split='', trueMasksPath=''):\n    if not os.path.exists(trueMasksPath):\n        os.makedirs(trueMasksPath)\n        print(f'{trueMasksPath} created')\n    image_path = os.path.join(trueMasksPath, f\"Frames_{frameType}/{split}/\")\n    mask_path = os.path.join(trueMasksPath, f\"Masks_{frameType}/{split}/\")\n    image_list = os.listdir(image_path)\n    mask_list = os.listdir(mask_path)\n    image_list = [image_path + i for i in image_list]\n    mask_list = [mask_path + i for i in mask_list]\n\n    image_filenames = tf.constant(image_list)\n    masks_filenames = tf.constant(mask_list)\n\n    dataset = tf.data.Dataset.from_tensor_slices((image_filenames, masks_filenames))\n    print(split, len(mask_list))\n\n    image_ds = dataset.map(_process_path)\n\n    return image_ds\n\n\n# Predict LandMask From Mask\ndef _getHorizontalLabel(mask):\n    upXY = (112, 0)\n\n    downXY_right = (0, 0)\n\n    downXY_left = (0, 112)\n\n    for x in range(112):\n        for y in range(112):\n            if mask[x][y] == 0:\n                continue\n\n            if upXY[0] > x and upXY[1] < y:\n                upXY = (x, y)\n            elif downXY_right[0] <= x and downXY_right[1] <= y:\n                downXY_right = (x, y)\n            elif downXY_left[0] < x or downXY_left[1] > y:\n                downXY_left = (x, y)\n\n    midpoint = ((downXY_left[0] + downXY_right[0]) // 2, (downXY_left[1] + downXY_right[1]) // 2)\n    for x in range(112):\n        for y in range(112):\n            midpoint = (midpoint[0] + 1, midpoint[1])\n\n            if midpoint[0] == 112 or mask[midpoint[0]][midpoint[1]] == 0:\n                midpoint = (midpoint[0] - 1, midpoint[1])\n                break\n\n    #     for i in range(112):\n    #         for j in range(112):\n    #             if mask[i][j] == 1:\n    #                 img[i][j] = (250, 250, 250)\n\n    # plt.scatter(upXY[1], upXY[0], color='orange', marker='o')\n    #\n    # plt.scatter(downXY_right[1], downXY_right[0], color='orange', marker='o')\n    #\n    # plt.scatter(downXY_left[1], downXY_left[0], color='orange', marker='o')\n    # plt.scatter(midpoint[1], midpoint[0], color='orange', marker='X')\n\n    if midpoint[1] == upXY[1]:\n        midpoint = (midpoint[0], midpoint[1] + 0.1)\n\n    return (midpoint[1], midpoint[0]), (upXY[1], upXY[0])\n\n\ndef _perpendicular_points(x1, y1, x2, y2, distance):\n    # Calculate slope of the original line\n    if x2 - x1 != 0:  # Avoid division by zero\n        slope_original = (y2 - y1) / (x2 - x1)\n        # Calculate negative reciprocal to get slope of perpendicular line\n        slope_perpendicular = -1 / slope_original\n    else:\n        slope_perpendicular = float('inf')  # Handle vertical lines\n\n    # Find midpoint of the original line\n\n    # Calculate unit vector along the original line\n    magnitude = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n    unit_vector_x = (x2 - x1) / magnitude\n    unit_vector_y = (y2 - y1) / magnitude\n\n    # Calculate displacement vector based on distance\n    displacement_x = unit_vector_x * distance\n    displacement_y = unit_vector_y * distance\n\n    # New midpoint for the perpendicular line\n    new_mid_x = x1 + displacement_x\n    new_mid_y = y1 + displacement_y\n\n    # Find points for the perpendicular line\n    dx = 1 / (1 + slope_perpendicular ** 2) ** 0.5\n    dy = slope_perpendicular * dx\n\n    # Two points for the perpendicular line\n    perpendicular_point1 = (new_mid_x + dx, new_mid_y + dy)\n    perpendicular_point2 = (new_mid_x - dx, new_mid_y - dy)\n\n    return perpendicular_point1, perpendicular_point2\n\n\ndef _find_previous_or_next_points(point1=None, point2=None, t='n'):\n    # Extract coordinates of the two points\n    x1, y1 = point1\n    x2, y2 = point2\n    x, y = 0, 0\n    # Calculate the distance between point1 and point2\n    distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n\n    # Calculate the slope of the line\n    if x2 - x1 != 0:  # Avoid division by zero\n        slope = (y2 - y1) / (x2 - x1)\n    else:\n        slope = None  # Line is vertical\n\n    if t == 'n':\n        # Calculate the next point\n        if slope is not None:\n            # If the line is not vertical, find next x and y\n            x = x2 + (x2 - x1) / distance\n            y = y2 + (y2 - y1) / distance\n        else:\n            # If the line is vertical, next point has the same x-coordinate\n            x = x2\n            y = y2 - distance\n    if t == 'p':\n        # Calculate the previous point\n        if slope is not None:\n            # If the line is not vertical, find previous x and y\n            x = x1 - (x2 - x1) / distance\n            y = y1 - (y2 - y1) / distance\n        else:\n            # If the line is vertical, previous point has the same x-coordinate\n            x = x1\n            y = y1 - distance\n\n    return x, y\n\n\ndef _getPointsInMask(point1, point2, mask):\n    # plt.plot([point1[0], point2[0]], [point1[1], point2[1]], marker='o', label='Points and Line')\n\n    pn = point2\n    lpn = None\n    for i in range(112):\n        if i == 0:\n            pn = _find_previous_or_next_points(point1, point2, t='n')\n            lpn = pn\n        x = int(np.round(pn[1]))\n        y = int(np.round(pn[0]))\n        if x == 112 or mask[x][y] == 0:\n            break\n        else:\n            lpn = pn\n            pn = _find_previous_or_next_points(point1, pn, t='n')\n        # plt.plot(pn[0], pn[1], marker='o', color='red', label='Next Point')\n\n    pn = lpn\n\n    pp = None\n    lpp = point1\n    for i in range(112):\n        if i == 0:\n            pp = _find_previous_or_next_points(point1, point2, t='p')\n        if int(np.round(pp[1])) == 112 or int(np.round(pp[0])) == 112 \\\n                or mask[int(np.round(pp[1]))][int(np.round(pp[0]))] == 0:\n            break\n        else:\n            lpp = pp\n            pp = _find_previous_or_next_points(pp, point2, t='p')\n        # plt.plot(pp[0], pp[1], marker='o', color='red')\n\n    pp = lpp\n\n    # plt.plot([pp[0], pn[0]], [pp[1], pn[1]], color='blue')\n\n    return pn, pp\n\n\ndef _GetPointOfSegmentMask(mask=None):\n    labels = [_getHorizontalLabel(mask)]\n\n    x1, y1 = labels[0][0]\n    x2, y2 = labels[0][1]\n\n    numOfLines = 20\n    distance = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) / numOfLines\n\n    point = []\n    for i in range(numOfLines):\n        point1, point2 = _perpendicular_points(x1, y1, x2, y2, distance * i)\n        point.append((point1, point2))\n\n        # plt.plot([x1, x2], [y1, y2], label=\"Original Line\")\n        # plt.plot([point1[0], point2[0]], [point1[1], point2[1]], label=\"Perpendicular Line\")\n        # plt.scatter([x1, x2], [y1, y2], color='red')\n        # plt.scatter([point1[0], point2[0]], [point1[1], point2[1]], color='blue')\n\n    for i in range(numOfLines):\n        labels.append(_getPointsInMask(point[i][0], point[i][1], mask))\n\n    #     for i in range(112):\n    #         for j in range(112):\n    #             if mask[i][j] == 1:\n    #                 img[i][j] = (250, 250, 250)\n\n    #     x = 'x'\n    #     for p1, p2 in labels:\n    #         plt.plot(p1[0], p1[1], marker=x, color='red', label='Next Point')\n    #         plt.plot(p2[0], p2[1], marker=x, color='red', label='Next Point')\n    #         plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color='blue')\n    #         x = 'o'\n\n    landmarks_pred = LandMarks([], [], [], [])\n\n    for i in range(21):\n        (x1, y1), (x2, y2) = labels[i]\n        landmarks_pred.X1.append(x1)\n        landmarks_pred.Y1.append(y1)\n\n        landmarks_pred.X2.append(x2)\n        landmarks_pred.Y2.append(y2)\n\n    # Arrange landmarks like Dataset\n    landmarks_pred.X1[0], landmarks_pred.X2[0] = landmarks_pred.X2[0], landmarks_pred.X1[0]\n    landmarks_pred.Y1[0], landmarks_pred.Y2[0] = landmarks_pred.Y2[0], landmarks_pred.Y1[0]\n\n    landmarks_pred.X1[1:21] = landmarks_pred.X1[1:21][::-1]\n    landmarks_pred.Y1[1:21] = landmarks_pred.Y1[1:21][::-1]\n\n    landmarks_pred.X2[1:21] = landmarks_pred.X2[1:21][::-1]\n    landmarks_pred.Y2[1:21] = landmarks_pred.Y2[1:21][::-1]\n\n    return landmarks_pred\n\n\n# Calc Volume and EF\ndef _calculate_volume(landmarks):\n    X1 = landmarks.X1\n    Y1 = landmarks.Y1\n    X2 = landmarks.X2\n    Y2 = landmarks.Y2\n    verticalLine_distance = math.sqrt((X2[0] - X1[0]) ** 2 + (Y2[0] - Y1[0]) ** 2)\n    dx = verticalLine_distance / 20\n\n    volume = 0\n    for i in range(1, 21):\n        volume += (math.pi * ((X2[i] - X1[i]) ** 2 + (Y2[i] - Y1[i]) ** 2) * dx) / 4.0\n\n    return volume\n\n\ndef calculate_EF(ED_volume, ES_volume):\n    return (abs(abs(ED_volume) - abs(ES_volume)) / ED_volume) * 100\n\n\ndef get_LV_volume(mask):\n    landmarks = _GetPointOfSegmentMask(mask)\n    volume = _calculate_volume(landmarks)\n    return volume, landmarks","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:10.673160Z","iopub.status.busy":"2024-04-28T18:47:10.672891Z","iopub.status.idle":"2024-04-28T18:47:10.731547Z","shell.execute_reply":"2024-04-28T18:47:10.730913Z"},"papermill":{"duration":0.086403,"end_time":"2024-04-28T18:47:10.733245","exception":false,"start_time":"2024-04-28T18:47:10.646842","status":"completed"},"tags":[]},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# DataSet Model","metadata":{"papermill":{"duration":0.053367,"end_time":"2024-04-28T18:47:10.810405","exception":false,"start_time":"2024-04-28T18:47:10.757038","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class VideoDataSetForModel(torch.utils.data.Dataset):\n    def __init__(self, dataSet=None, fullVideo=False):\n\n        self.dataSet = dataSet\n        self.fullVideo = fullVideo\n        self.frame_width = self.frame_height = 128\n\n    def __len__(self):\n        return len(self.dataSet)\n\n    def __getitem__(self, index):\n        obj = self.dataSet[index]\n\n        if self.fullVideo:\n            frame_count = obj.numberOfFrames\n            path = os.path.join(_videosPath, obj.fileName + '.avi')\n            video = _extractVideoFrames(path)\n        else:\n            frame_count = 128\n            video = _mirroringVideo(obj)\n\n        frames = np.zeros((frame_count, 112, 112, 3), np.float32)\n        labels = np.zeros(frame_count, np.int8)\n\n        if self.fullVideo:\n            frames = video\n            labels[obj.ES_frame] = 1\n            labels[obj.ED_frame] = 2\n        else:\n            for i in range(0, frame_count):\n                # 0 TR , 1 ES, 2 ED\n                label = video[i][1]\n                if label == 'ES':\n                    label = 1\n                elif label == 'ED':\n                    label = 2\n                else:\n                    label = 0\n\n                frames[i] = video[i][0]\n                labels[i] = label\n\n        # (F,W,H,C) > F C W H\n        frames = frames.transpose((3, 0, 1, 2))\n\n        ########################\n        # Load video into np.array\n        frames = frames.astype(np.float32)\n\n        # Scale pixel values from 0-255 to 0-1\n        frames /= 255.0\n\n        frames = np.moveaxis(frames, 0, 1)\n        p = 8\n        frames = np.pad(frames, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant', constant_values=0)\n        ########################\n\n        return frames, labels","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:10.859377Z","iopub.status.busy":"2024-04-28T18:47:10.858990Z","iopub.status.idle":"2024-04-28T18:47:10.867501Z","shell.execute_reply":"2024-04-28T18:47:10.866869Z"},"papermill":{"duration":0.035181,"end_time":"2024-04-28T18:47:10.869353","exception":false,"start_time":"2024-04-28T18:47:10.834172","status":"completed"},"tags":[]},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Transformer","metadata":{"papermill":{"duration":0.023448,"end_time":"2024-04-28T18:47:10.916376","exception":false,"start_time":"2024-04-28T18:47:10.892928","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"ResNet Model","metadata":{"papermill":{"duration":0.02337,"end_time":"2024-04-28T18:47:10.964038","exception":false,"start_time":"2024-04-28T18:47:10.940668","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ResidualBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1)):\n        super(ResidualBlock, self).__init__()\n\n        self.residual_block = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                            padding=1),\n            torch.nn.BatchNorm2d(in_channels),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                            padding=1),\n            torch.nn.BatchNorm2d(out_channels),\n            torch.nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return x + self.residual_block(x)\n\n\nclass ResNetEncoder(torch.nn.Module):\n    def __init__(self,\n                 n_ResidualBlock=8,\n                 n_levels=4,\n                 input_ch=3,\n                 z_dim=10,\n                 bUseMultiResSkips=True):\n\n        super(ResNetEncoder, self).__init__()\n\n        self.max_filters = 2 ** (n_levels + 3)\n        self.n_levels = n_levels\n        self.bUseMultiResSkips = bUseMultiResSkips\n\n        self.conv_list = torch.nn.ModuleList()\n        self.res_blk_list = torch.nn.ModuleList()\n        self.multi_res_skip_list = torch.nn.ModuleList()\n\n        self.input_conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=input_ch, out_channels=8,\n                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n            torch.nn.BatchNorm2d(8),\n            torch.nn.ReLU(inplace=True),\n        )\n\n        for i in range(n_levels):\n            n_filters_1 = 2 ** (i + 3)\n            n_filters_2 = 2 ** (i + 4)\n            ks = 2 ** (n_levels - i)\n\n            self.res_blk_list.append(\n                torch.nn.Sequential(*[ResidualBlock(n_filters_1, n_filters_1)\n                                      for _ in range(n_ResidualBlock)])\n            )\n\n            self.conv_list.append(\n                torch.nn.Sequential(\n                    torch.nn.Conv2d(n_filters_1, n_filters_2,\n                                    kernel_size=(2, 2), stride=(2, 2), padding=0),\n                    torch.nn.BatchNorm2d(n_filters_2),\n                    torch.nn.ReLU(inplace=True),\n                )\n            )\n\n            if bUseMultiResSkips:\n                self.multi_res_skip_list.append(\n                    torch.nn.Sequential(\n                        torch.nn.Conv2d(in_channels=n_filters_1, out_channels=self.max_filters, kernel_size=(ks, ks),\n                                        stride=(ks, ks), padding=0),\n                        torch.nn.BatchNorm2d(self.max_filters),\n                        torch.nn.ReLU(inplace=True),\n                    )\n                )\n\n        self.output_conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=self.max_filters, out_channels=z_dim,\n                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n            torch.nn.BatchNorm2d(z_dim),\n            torch.nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n\n        x = self.input_conv(x)\n\n        skips = []\n        for i in range(self.n_levels):\n            x = self.res_blk_list[i](x)\n            if self.bUseMultiResSkips:\n                skips.append(self.multi_res_skip_list[i](x))\n            x = self.conv_list[i](x)\n\n        if self.bUseMultiResSkips:\n            x = sum([x] + skips)\n\n        x = self.output_conv(x)\n\n        return x\n\n\nclass ResNetDecoder(torch.nn.Module):\n    def __init__(self,\n                 n_ResidualBlock=8,\n                 n_levels=4,\n                 z_dim=10,\n                 output_channels=3,\n                 bUseMultiResSkips=True):\n\n        super(ResNetDecoder, self).__init__()\n\n        self.max_filters = 2 ** (n_levels + 3)\n        self.n_levels = n_levels\n        self.bUseMultiResSkips = bUseMultiResSkips\n\n        self.conv_list = torch.nn.ModuleList()\n        self.res_blk_list = torch.nn.ModuleList()\n        self.multi_res_skip_list = torch.nn.ModuleList()\n\n        self.input_conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=z_dim, out_channels=self.max_filters,\n                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n            torch.nn.BatchNorm2d(self.max_filters),\n            torch.nn.ReLU(inplace=True),\n        )\n\n        n_filters_1 = 2 ** (self.n_levels - 0 + 2)\n        for i in range(n_levels):\n            n_filters_0 = 2 ** (self.n_levels - i + 3)\n            n_filters_1 = 2 ** (self.n_levels - i + 2)\n            ks = 2 ** (i + 1)\n\n            self.res_blk_list.append(\n                torch.nn.Sequential(*[ResidualBlock(n_filters_1, n_filters_1)\n                                      for _ in range(n_ResidualBlock)])\n            )\n\n            self.conv_list.append(\n                torch.nn.Sequential(\n                    torch.nn.ConvTranspose2d(n_filters_0, n_filters_1,\n                                             kernel_size=(2, 2), stride=(2, 2), padding=0),\n                    torch.nn.BatchNorm2d(n_filters_1),\n                    torch.nn.ReLU(inplace=True),\n                )\n            )\n\n            if bUseMultiResSkips:\n                self.multi_res_skip_list.append(\n                    torch.nn.Sequential(\n                        torch.nn.ConvTranspose2d(in_channels=self.max_filters, out_channels=n_filters_1,\n                                                 kernel_size=(ks, ks), stride=(ks, ks), padding=0),\n                        torch.nn.BatchNorm2d(n_filters_1),\n                        torch.nn.ReLU(inplace=True),\n                    )\n                )\n\n        self.output_conv = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=n_filters_1, out_channels=output_channels,\n                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n            # torch.nn.BatchNorm2d(output_channels),\n            # torch.nn.ReLU(inplace=True),\n        )\n\n    def forward(self, z):\n\n        z = z_top = self.input_conv(z)\n\n        for i in range(self.n_levels):\n            z = self.conv_list[i](z)\n            z = self.res_blk_list[i](z)\n            if self.bUseMultiResSkips:\n                z += self.multi_res_skip_list[i](z_top)\n\n        z = self.output_conv(z)\n\n        return z\n\n\nclass ResNetAE(torch.nn.Module):\n    def __init__(self,\n                 input_shape=(256, 256, 3),\n                 n_ResidualBlock=8,\n                 n_levels=4,\n                 z_dim=128,\n                 bottleneck_dim=128,\n                 bUseMultiResSkips=True):\n        super(ResNetAE, self).__init__()\n\n        assert input_shape[0] == input_shape[1]\n        image_channels = input_shape[2]\n        self.z_dim = z_dim\n        self.img_latent_dim = input_shape[0] // (2 ** n_levels)\n\n        self.encoder = ResNetEncoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n                                     input_ch=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n        self.decoder = ResNetDecoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n                                     output_channels=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n\n        self.fc1 = torch.nn.Linear(self.z_dim * self.img_latent_dim * self.img_latent_dim, bottleneck_dim)\n        self.fc2 = torch.nn.Linear(bottleneck_dim, self.z_dim * self.img_latent_dim * self.img_latent_dim)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        return torch.tanh(self.fc1(h.view(x.shape[0], self.z_dim * self.img_latent_dim * self.img_latent_dim)))\n\n    def decode(self, z):\n        h = self.decoder(self.fc2(z).view(-1, self.z_dim, self.img_latent_dim, self.img_latent_dim))\n        return torch.sigmoid(h)\n\n    def forward(self, x):\n        return self.decode(self.encode(x))\n\n\ndef reParameterize(mu, log):\n    std = torch.exp(0.5 * log)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\n\nclass ResNetVAE(torch.nn.Module):\n    def __init__(self,\n                 input_shape=(256, 256, 3),\n                 n_ResidualBlock=8,\n                 n_levels=4,\n                 z_dim=128,\n                 bottleneck_dim=128,\n                 bUseMultiResSkips=True):\n        super(ResNetVAE, self).__init__()\n\n        assert input_shape[0] == input_shape[1]\n        image_channels = input_shape[2]\n        self.z_dim = z_dim\n        self.img_latent_dim = input_shape[0] // (2 ** n_levels)\n\n        self.encoder = ResNetEncoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n                                     input_ch=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n        self.decoder = ResNetDecoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n                                     output_channels=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n\n        # Assumes the input to be of shape 256x256\n        self.fc21 = torch.nn.Linear(self.z_dim * self.img_latent_dim * self.img_latent_dim, bottleneck_dim)\n        self.fc22 = torch.nn.Linear(self.z_dim * self.img_latent_dim * self.img_latent_dim, bottleneck_dim)\n        self.fc3 = torch.nn.Linear(bottleneck_dim, self.z_dim * self.img_latent_dim * self.img_latent_dim)\n\n    def encode(self, x):\n        h1 = self.encoder(x)\n        return self.fc21(h1.view(-1, self.z_dim * self.img_latent_dim * self.img_latent_dim)), \\\n            self.fc22(h1.view(-1, self.z_dim * self.img_latent_dim * self.img_latent_dim))\n\n    def decode(self, z):\n        h3 = self.decoder(self.fc3(z).view(-1, self.z_dim, self.img_latent_dim, self.img_latent_dim))\n        return torch.sigmoid(h3)\n\n    def forward(self, x):\n        mu, log = self.encode(x)\n        z = reParameterize(mu, log)\n        return self.decode(z), mu, log","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.013460Z","iopub.status.busy":"2024-04-28T18:47:11.013139Z","iopub.status.idle":"2024-04-28T18:47:11.044307Z","shell.execute_reply":"2024-04-28T18:47:11.043650Z"},"papermill":{"duration":0.05843,"end_time":"2024-04-28T18:47:11.045819","exception":false,"start_time":"2024-04-28T18:47:10.987389","status":"completed"},"tags":[]},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Transformer Model","metadata":{"papermill":{"duration":0.023462,"end_time":"2024-04-28T18:47:11.092981","exception":false,"start_time":"2024-04-28T18:47:11.069519","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self, embedding_dim=1024, num_hidden_layers=16, attention_heads=16, intermediate_size=8192,\n                 input_shape=(128, 128, 3)):\n        super(TransformerModel, self).__init__()\n\n        # ResNet encoder\n        self.model_AE = ResNetAE(input_shape=input_shape, n_ResidualBlock=8, n_levels=4,\n                                 bottleneck_dim=embedding_dim)\n        self.model_AE.decoder = None\n        self.model_AE.fc2 = None\n\n        # BertModel encoder\n        configuration = BertConfig(\n            vocab_size=1,  # Set to 0/None ?\n            hidden_size=embedding_dim,  # Length of embeddings\n            num_hidden_layers=num_hidden_layers,  # 16\n            num_attention_heads=attention_heads,\n            intermediate_size=intermediate_size,  # 8192\n            hidden_act='gelu',\n            hidden_dropout_prob=0.1,\n            attention_probs_dropout_prob=0.1,\n            max_position_embeddings=1024,  # 64 ?\n            type_vocab_size=1,\n            initializer_range=0.02,\n            layer_norm_eps=1e-12,\n            pad_token_id=0,\n            gradient_checkpointing=False,\n            position_embedding_type='absolute',\n            use_cache=True)\n\n        configuration.num_labels = 3\n\n        self.model_Bert = BertModel(configuration).encoder\n\n        self.embedding_dim = embedding_dim\n\n        last_features = 3\n        self.extremas = nn.Sequential(\n            nn.Linear(in_features=embedding_dim, out_features=embedding_dim // 2, bias=True),\n            nn.LayerNorm(embedding_dim // 2),\n            nn.LeakyReLU(negative_slope=0.05, inplace=True),\n\n            nn.Linear(in_features=embedding_dim // 2, out_features=embedding_dim // 4, bias=True),\n            nn.LayerNorm(embedding_dim // 4),\n            nn.LeakyReLU(negative_slope=0.05, inplace=True),\n\n            nn.Linear(in_features=embedding_dim // 4, out_features=last_features, bias=True),\n            nn.Tanh()\n        )\n\n    def forward(self, frames, nF):\n        # (BxF) x C x H x W => (BxF) x Emb\n        # Frame [128, 3, 128, 128]\n        frames = self.model_AE.encode(frames).squeeze()\n        # embeddings [128, 1024]\n        # output_bert\n        # B x F x Emb => AttHeads+1 x B x F x Emb\n        frames = self.model_Bert(frames.view(-1, nF, self.embedding_dim), output_hidden_states=True)\n        # AttHeads+1 x B x F x Emb => B x F x Emb\n        frames = torch.stack(frames.hidden_states).mean(dim=0)\n        # B x F x Emb => B x F x 1\n        frames = self.extremas(frames)\n        # classes_vec [1, 128, 3]\n        return frames","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.143361Z","iopub.status.busy":"2024-04-28T18:47:11.142990Z","iopub.status.idle":"2024-04-28T18:47:11.153190Z","shell.execute_reply":"2024-04-28T18:47:11.152461Z"},"papermill":{"duration":0.038485,"end_time":"2024-04-28T18:47:11.155007","exception":false,"start_time":"2024-04-28T18:47:11.116522","status":"completed"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Transformer Function","metadata":{"papermill":{"duration":0.029662,"end_time":"2024-04-28T18:47:11.215062","exception":false,"start_time":"2024-04-28T18:47:11.185400","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n\ndef run_epoch(model, dataloader, optim, device):\n    total = 0.\n    n = 0\n    loss_hist = []\n\n    model.train(True)\n    print(\"Learning rate:\", get_lr(optim))\n\n    weighting = torch.tensor([1., 5., 5.]).to(device)\n    loss_fct1 = nn.CrossEntropyLoss(weight=weighting, reduction='mean')\n\n    with tqdm.tqdm(total=len(dataloader)) as pbar:\n        for (frames, label) in dataloader:\n            nB, nF, nC, nH, nW = frames.shape\n\n            # Merge batch and frames dimension\n            frames = frames.view(nB * nF, nC, nH, nW)\n            frames = frames.to(device, dtype=torch.float32)\n\n            # (F*B) X C x W X H\n            class_vec = model(frames, nF)\n\n            label = label.to(device, dtype=torch.long)\n\n            loss1 = loss_fct1(class_vec.view(-1, 3), label.view(-1))\n            loss = loss1\n\n            # Take gradient step if training\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n            # Accumulate losses and compute baselines\n            total += loss.item()\n            n += 1\n            loss_hist.append(loss.item())\n\n            avg = np.mean(loss_hist[max(-len(loss_hist), -10):])\n\n            # Show info on process bar\n            pbar.set_postfix_str(\"{:.4f} / {:.4f} / {:.4f}\".format(total / n, loss1.item(), avg))\n            pbar.update()\n    loss_hist = np.array(loss_hist)\n\n    return (total / 1), loss_hist\n\n\ndef trainTransformer(train_dataSet, num_epochs, batch_size, parallel):\n    np.random.seed(0)\n    torch.manual_seed(0)\n\n    # Device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if type(device) == type(list()):\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in device)\n        device = \"cuda\"\n\n    device = torch.device(device)\n    print(\"Using device:\", device)\n\n    # Model\n    model = TransformerModel(embedding_dim=1024, num_hidden_layers=16, attention_heads=16, intermediate_size=8192,\n                             input_shape=(128, 128, 3))\n\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print('model name', model.__class__.__name__, \"contains\", pytorch_total_params, \"parameters.\")\n\n    if parallel:\n        model = nn.DataParallel(model)\n\n    model.to(device)\n\n    # DATA SETUP\n    train_dataSet = VideoDataSetForModel(dataSet=train_dataSet, fullVideo=False)\n\n    train_dataloader = torch.utils.data.DataLoader(train_dataSet, batch_size=batch_size, num_workers=0,\n                                                   shuffle=True,\n                                                   pin_memory=(device.type == \"cuda\"),\n                                                   drop_last=True)\n\n    dataloaders = {'train': train_dataloader}\n    # len(dataloaders['train'])\n\n    # Set up optimizer\n    lr = 1e-5\n    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_step_period = 1\n    scheduler = torch.optim.lr_scheduler.StepLR(optim, lr_step_period)\n\n    bestLoss = float(\"inf\")\n\n    for epoch in range(1, num_epochs + 1):\n        print(\"Epoch {} / {}\".format(epoch, num_epochs), flush=True)\n        for phase in ['train']:  # , 'val']:\n            print(\"Running on\", phase)\n            loss, _ = run_epoch(model, dataloaders[phase], optim, device)\n            print('Loss =', loss)\n            print()\n        scheduler.step()\n\n        # Save checkpoint\n        save = {\n            'epoch': epoch,\n            'state_dict': model.state_dict(),\n            'best_loss': bestLoss,\n            'loss': loss,\n            'opt_dict': optim.state_dict(),\n            'scheduler_dict': scheduler.state_dict(),\n        }\n        if loss < bestLoss:\n            print('new Best Version')\n            torch.save(save, \"best.pt\")\n            bestLoss = loss\n        else:\n            torch.save(save, \"checkpoint_\" + str(epoch) + \".pt\")\n\n\n# Test\ndef show_graph(label, predict):\n    predict = predict.copy() + 0.5\n    plt.plot(predict, label='predict')\n    plt.plot(label, label='Label')\n    plt.legend()\n    plt.show()\n\n\ndef smooth(vec, window=5, rep=1):\n    weight = torch.ones((1, 1, window)) / window\n    for _ in range(rep):\n        pad = int((window - 1) / 2)\n        vec = vec.unsqueeze(0).unsqueeze(0)\n        vec = torch.nn.functional.conv1d(vec, weight, bias=None, stride=1, padding=pad, dilation=1, groups=1).squeeze()\n    return vec\n\n\ndef loadTransformerModel(path):\n    best = torch.load(path, map_location=\"cpu\")\n    model = TransformerModel(embedding_dim=1024, num_hidden_layers=16, attention_heads=16, intermediate_size=8192,\n                             input_shape=(128, 128, 3))\n    model = torch.nn.DataParallel(model)\n    model.load_state_dict(best['state_dict'])\n    model.eval()\n    return model\n\n\ndef GetLengthOfEachBet(predict, deleteLastHalfBet=False):\n    lengthBet = []\n    firstFrames = []\n    lastFrames = []\n    check = True\n\n    for i in range(len(predict)):\n        if predict[i] != 0:\n            if check:\n                firstFrames.append(i)\n                check = False\n            else:\n                lastFrames.append(i)\n                check = True\n\n    if deleteLastHalfBet:\n        # Delete the last half Bet\n        if len(firstFrames) > len(lastFrames):\n            predict[firstFrames[-1]] = 0\n            firstFrames.pop()\n        elif len(firstFrames) < len(lastFrames):\n            predict[lastFrames[-1]] = 0\n            lastFrames.pop()\n\n    for i in range(len(firstFrames)):\n        lengthBet.append(lastFrames[i] - firstFrames[i])\n\n    return lengthBet, firstFrames, lastFrames\n\n\ndef testForOneVideo(model, frames, device):\n    nB, nF, nC, nH, nW = frames.shape\n    frames = torch.cat(([frames[i] for i in range(frames.size(0))]), dim=0)\n    frames = frames.to(device, dtype=torch.float)\n\n    class_vec = model(frames, nF).squeeze()\n\n    class_diff = class_vec[:, 2] - class_vec[:, 1]\n\n    smooth_vec = smooth(class_diff, window=5, rep=3).detach().numpy()\n\n    # Get Peaks\n    predict = np.zeros((len(smooth_vec)), np.int8)\n    for i in range(len(smooth_vec)):\n        if i == 0 or i == len(smooth_vec) - 1:\n            continue\n        if smooth_vec[i] < smooth_vec[i + 1] and smooth_vec[i] < smooth_vec[i - 1]:\n            predict[i] = 1\n        if smooth_vec[i] > smooth_vec[i + 1] and smooth_vec[i] > smooth_vec[i - 1]:\n            predict[i] = 2\n\n    # Get length of each bet\n    lengthBet, firstFrames, lastFrames = GetLengthOfEachBet(predict, True)\n\n    # Apply Threshold\n    thr = max(lengthBet) * 0.35\n    for i in range(len(lengthBet)):\n        if thr > lengthBet[i]:\n            predict[firstFrames[i]] = 0\n            predict[lastFrames[i]] = 0\n\n    return predict\n\n\ndef testTransformer(transformer_path, dataSet):\n    device = 'cpu'\n    device = torch.device(device)\n    model = loadTransformerModel(transformer_path)\n\n    dataSet = VideoDataSetForModel(dataSet=dataSet, fullVideo=True)\n    dataloader = torch.utils.data.DataLoader(dataSet, batch_size=1, shuffle=False)\n    trueFrames = 0\n    trueTransitionFrames = 0\n    trueESFrames = 0\n    trueEDFrames = 0\n    totalFrames = 0\n    trueFrames2 = 0\n    with tqdm.tqdm(total=len(dataloader)) as pbar:\n        for frames, label in dataloader:\n            predict = testForOneVideo(model, frames, device)\n            label = label.squeeze().detach().numpy()\n\n            totalFrames += len(label)\n            for i in range(len(label)):\n                if predict[i] == label[i]:\n                    trueFrames += 1\n                if predict[i] == 0 and label[i] == 0:\n                    trueTransitionFrames += 1\n                elif predict[i] == 1 and label[i] == 1:\n                    trueEDFrames += 1\n                elif predict[i] == 2 and label[i] == 2:\n                    trueESFrames += 1\n\n                if label[i] != 0:\n                    if predict[i] == label[i]:\n                        trueFrames2 += 1\n\n            pbar.update()\n\n    accuracy = (trueFrames / totalFrames) * 100\n    print('Accuracy: ', accuracy)\n\n    accuracy2 = (trueFrames2 / (2 * len(dataloader))) * 100\n    print('Accuracy ES & ED: ', accuracy2)\n\n    accuracyED = (trueEDFrames / len(dataloader)) * 100\n    print('Accuracy ED: ', accuracyED)\n\n    accuracyES = (trueESFrames / len(dataloader)) * 100\n    print('Accuracy ES: ', accuracyES)\n\n    accuracyTransition = (trueTransitionFrames / (totalFrames - len(dataloader) * 2)) * 100\n    print('Accuracy Transition: ', accuracyTransition)\n\n\n# Detect ES & ED Frame\ndef Detect_ESED_Frame(video_path, transformerModel, labels=None):\n    device = 'cpu'\n    device = torch.device(device)\n    # Prepare Video\n    frames = _extractVideoFrames(video_path)\n    # (F,W,H,C) > F C W H\n    frames = frames.transpose((3, 0, 1, 2))\n    # Load video into np.array\n    frames = frames.astype(np.float32)\n    # Scale pixel values from 0-255 to 0-1\n    frames /= 255.0\n\n    frames = np.moveaxis(frames, 0, 1)\n    p = 8\n    frames = np.pad(frames, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant', constant_values=0)\n\n    frames = torch.from_numpy(frames)\n    frames = frames.unsqueeze(0)\n\n    predict = testForOneVideo(transformerModel, frames, device)\n\n    lengthBet, firstFrames, lastFrames = GetLengthOfEachBet(predict, False)\n\n    maxIDX = 0\n    for i in range(1, len(lengthBet)):\n        if lengthBet[maxIDX] <= lengthBet[i]:\n            maxIDX = i\n\n    frames = frames.squeeze()\n    if predict[firstFrames[maxIDX]] == 1:\n        ES_Frame_IMG = np.transpose(frames[firstFrames[maxIDX]], (1, 2, 0))\n        ED_Frame_IMG = np.transpose(frames[lastFrames[maxIDX]], (1, 2, 0))\n    else:\n        ED_Frame_IMG = np.transpose(frames[firstFrames[maxIDX]], (1, 2, 0))\n        ES_Frame_IMG = np.transpose(frames[lastFrames[maxIDX]], (1, 2, 0))\n\n    # Show 4 Frames\n    if labels is not None:\n        print(firstFrames[maxIDX], lastFrames[maxIDX])\n        TrueES_Frame = 0\n        TrueED_Frame = 0\n        for i in range(len(labels)):\n            if labels[i] == 1:\n                TrueES_Frame = np.transpose(frames[i], (1, 2, 0))\n            elif labels[i] == 2:\n                TrueED_Frame = np.transpose(frames[i], (1, 2, 0))\n\n            fig, axes = plt.subplots(2, 2)\n\n            axes[0][0].imshow(ES_Frame_IMG)\n            axes[0][0].set_title('ES Pred')\n            axes[0][0].axis('off')\n\n            axes[0][1].imshow(TrueES_Frame)\n            axes[0][1].set_title('ES True')\n            axes[0][1].axis('off')\n\n            axes[1][0].imshow(ED_Frame_IMG)\n            axes[1][0].set_title('ED Pred')\n            axes[1][0].axis('off')\n\n            axes[1][1].imshow(TrueED_Frame)\n            axes[1][1].set_title('ED True')\n            axes[1][1].axis('off')\n\n            plt.show()\n\n    # Crop the padding added in train\n    ES_Frame_IMG = ES_Frame_IMG.numpy()[8:-8, 8:-8, :]\n    ED_Frame_IMG = ED_Frame_IMG.numpy()[8:-8, 8:-8, :]\n\n    return ES_Frame_IMG, ED_Frame_IMG","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.266508Z","iopub.status.busy":"2024-04-28T18:47:11.266168Z","iopub.status.idle":"2024-04-28T18:47:11.309301Z","shell.execute_reply":"2024-04-28T18:47:11.308369Z"},"papermill":{"duration":0.0709,"end_time":"2024-04-28T18:47:11.311459","exception":false,"start_time":"2024-04-28T18:47:11.240559","status":"completed"},"tags":[]},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Unet","metadata":{"papermill":{"duration":0.031034,"end_time":"2024-04-28T18:47:11.369887","exception":false,"start_time":"2024-04-28T18:47:11.338853","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Unet Model","metadata":{"papermill":{"duration":0.028398,"end_time":"2024-04-28T18:47:11.427618","exception":false,"start_time":"2024-04-28T18:47:11.399220","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# UNQ_C1\n# GRADED FUNCTION: conv_block\ndef conv_block(inputs=None, n_filters=32, dropout_prob=0, max_pooling=True):\n    \"\"\"\n    Convolutional downsampling block\n\n    Arguments:\n        inputs -- Input tensor\n        n_filters -- Number of filters for the convolutional layers\n        dropout_prob -- Dropout probability\n        max_pooling -- Use MaxPooling2D to reduce the spatial dimensions of the output volume\n    Returns:\n        next_layer, skip_connection --  Next layer and skip connection outputs\n    \"\"\"\n\n    ### START CODE HERE\n    conv = Conv2D(n_filters,  # Number of filters\n                  3,  # Kernel size\n                  activation='relu',\n                  padding='same',\n                  kernel_initializer='he_normal')(inputs)\n    conv = Conv2D(n_filters,  # Number of filters\n                  3,  # Kernel size\n                  activation='relu',\n                  padding='same',\n                  kernel_initializer='he_normal')(conv)\n    ### END CODE HERE\n\n    # if dropout_prob > 0 add a dropout layer, with the variable dropout_prob as parameter\n    if dropout_prob > 0:\n        ### START CODE HERE\n        conv = Dropout(dropout_prob)(conv)\n        ### END CODE HERE\n\n    # if max_pooling is True add a MaxPooling2D with 2x2 pool_size\n    if max_pooling:\n        ### START CODE HERE\n        next_layer = MaxPooling2D(2, strides=2)(conv)\n        ### END CODE HERE\n\n    else:\n        next_layer = conv\n\n    skip_connection = conv\n\n    return next_layer, skip_connection\n\n\n# UNQ_C2\n# GRADED FUNCTION: upsampling_block\ndef upsampling_block(expansive_input, contractive_input, n_filters=32):\n    \"\"\"\n    Convolutional upsampling block\n\n    Arguments:\n        expansive_input -- Input tensor from previous layer\n        contractive_input -- Input tensor from previous skip layer\n        n_filters -- Number of filters for the convolutional layers\n    Returns:\n        conv -- Tensor output\n    \"\"\"\n\n    ### START CODE HERE\n    up = Conv2DTranspose(\n        n_filters,  # number of filters\n        3,  # Kernel size\n        strides=2,\n        padding='same')(expansive_input)\n\n    # Merge the previous output and the contractive_input\n    merge = concatenate([up, contractive_input], axis=3)\n\n    conv = Conv2D(n_filters,  # Number of filters\n                  3,  # Kernel size\n                  activation='relu',\n                  padding='same',\n                  kernel_initializer='he_normal')(merge)\n    conv = Conv2D(n_filters,  # Number of filters\n                  3,  # Kernel size\n                  activation='relu',\n                  padding='same',\n                  kernel_initializer='he_normal')(conv)\n    ### END CODE HERE\n\n    return conv\n\n\n# UNQ_C3\n# GRADED FUNCTION: unet_model\ndef unetModel(input_size, n_filters, n_classes):\n    \"\"\"\n    Unet model\n\n    Arguments:\n        input_size -- Input shape\n        n_filters -- Number of filters for the convolutional layers\n        n_classes -- Number of output classes\n    Returns:\n        model -- tf.keras.Model\n    \"\"\"\n    inputs = Input(input_size)\n    # Contracting Path (encoding)\n    # Add a conv_block with the inputs of the unet_ model and n_filters\n    ### START CODE HERE\n    cblock1 = conv_block(inputs=inputs, n_filters=n_filters * 1)\n    # Chain the first element of the output of each block to be the input of the next conv_block.\n    # Double the number of filters at each new step\n    cblock2 = conv_block(inputs=cblock1[0], n_filters=n_filters * 2)\n    cblock3 = conv_block(inputs=cblock2[0], n_filters=n_filters * 4)\n    # Include a dropout of 0.3 for this layer\n    cblock4 = conv_block(inputs=cblock3[0], n_filters=n_filters * 8, dropout_prob=0.3)\n    # Include a dropout of 0.3 for this layer, and avoid the max_pooling layer\n    cblock5 = conv_block(inputs=cblock4[0], n_filters=n_filters * 16, dropout_prob=0.3, max_pooling=False)\n    ### END CODE HERE\n\n    # Expanding Path (decoding)\n    # Add the first upsampling_block.\n    # From here,at each step, use half the number of filters of the previous block\n    # Use the cblock5[0] as expansive_input and cblock4[1] as contractive_input and n_filters * 8\n    ### START CODE HERE\n    ublock6 = upsampling_block(cblock5[0], cblock4[1], n_filters * 8)\n    # Chain the output of the previous block as expansive_input and the corresponding contractive block output.\n    # Note that you must use the second element of the contractive block i.e before the maxpooling layer.\n\n    ublock7 = upsampling_block(ublock6, cblock3[1], n_filters * 4)\n    ublock8 = upsampling_block(ublock7, cblock2[1], n_filters * 2)\n    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters * 1)\n    ### END CODE HERE\n\n    conv9 = Conv2D(n_filters,\n                   3,\n                   activation='relu',\n                   padding='same',\n                   kernel_initializer='he_normal')(ublock9)\n\n    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\n    ### START CODE HERE\n    conv10 = Conv2D(n_classes, 1, padding='same')(conv9)\n    ### END CODE HERE\n\n    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n\n    return model","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.483907Z","iopub.status.busy":"2024-04-28T18:47:11.483544Z","iopub.status.idle":"2024-04-28T18:47:11.496011Z","shell.execute_reply":"2024-04-28T18:47:11.495311Z"},"papermill":{"duration":0.042425,"end_time":"2024-04-28T18:47:11.497647","exception":false,"start_time":"2024-04-28T18:47:11.455222","status":"completed"},"tags":[]},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Unet Function","metadata":{"papermill":{"duration":0.024577,"end_time":"2024-04-28T18:47:11.547431","exception":false,"start_time":"2024-04-28T18:47:11.522854","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def predictLVForEDESFrames(ES_Frame_IMG, ED_Frame_IMG, ED_model, ES_model):\n    # Mask\n    ED_pred_mask = predictMask(ED_model, np.expand_dims(ED_Frame_IMG, axis=0))\n    ES_pred_mask = predictMask(ES_model, np.expand_dims(ES_Frame_IMG, axis=0))\n\n    # Volume\n    ED_pred_volume, ED_pred_landmarks = get_LV_volume(ED_pred_mask)\n    ES_pred_volume, ES_pred_landmarks = get_LV_volume(ES_pred_mask)\n\n    # EF\n    ef_pred = calculate_EF(ED_pred_volume, ES_pred_volume)\n\n    return ef_pred\n\n\ndef calculate_mean_mse(ground_truth_masks, predicted_masks):\n    num_masks = len(predicted_masks)\n    mse_values = []\n\n    for i in range(num_masks):\n        mse = mean_squared_error(ground_truth_masks[i].flatten(), predicted_masks[i].flatten())\n        mse_values.append(mse)\n\n    mean_mse = np.mean(mse_values)\n    print(f\"Mean MSE: {mean_mse * 100}\")\n    # return mean_mse\n\n\ndef _createPredictedMask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]\n\n\ndef predictMask(model, image):\n    mask = _createPredictedMask(model.predict(image))\n    mask = mask.numpy()\n    mask = np.squeeze(mask)\n    return mask\n\n\ndef loadUnetModel(path):\n    loaded_model = unetModel(input_size=(112, 112, 3), n_filters=32, n_classes=2)\n    loaded_model.load_weights(path)\n    loaded_model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n    # print(loaded_model.get_weights()[0][0][0][0])\n\n    return loaded_model\n\n\ndef trainUnet(dataSet, epochs=5, batchSize=32, modelPath='', name=''):\n    unet = unetModel(input_size=(112, 112, 3), n_filters=32, n_classes=2)\n    unet.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True),\n                 metrics=['accuracy'])\n    # unet.summary()\n\n    BUFFER_SIZE = 500\n    dataSet.batch(batchSize)\n    dataSet = dataSet.cache().shuffle(BUFFER_SIZE).batch(batchSize)\n\n    unet.fit(dataSet, epochs=epochs)\n\n    unet.save_weights(f'{modelPath}/{name}.weights.h5')\n\n\ndef testUnet(dataSet, path=''):\n    dataSet.batch(1)\n    dataSet = dataSet.cache().batch(1)\n\n    loaded_model = loadUnetModel(path)\n\n    evaluation_result = loaded_model.evaluate(dataSet)\n    print(\"Test Accuracy:\", evaluation_result[1])","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.602494Z","iopub.status.busy":"2024-04-28T18:47:11.601951Z","iopub.status.idle":"2024-04-28T18:47:11.612879Z","shell.execute_reply":"2024-04-28T18:47:11.612201Z"},"papermill":{"duration":0.040042,"end_time":"2024-04-28T18:47:11.614586","exception":false,"start_time":"2024-04-28T18:47:11.574544","status":"completed"},"tags":[]},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Train & Test","metadata":{"papermill":{"duration":0.024254,"end_time":"2024-04-28T18:47:11.662914","exception":false,"start_time":"2024-04-28T18:47:11.638660","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # Read Data\ntrain_dataSet = load_or_get_data(spilt_type='TRAIN')\nprint('TRAIN =', len(train_dataSet))\n\ntest_dataSet = load_or_get_data(spilt_type='TEST')\nprint('TEST =', len(test_dataSet))\n\nval_dataSet = load_or_get_data(spilt_type='VAL')\nprint('VAL =', len(val_dataSet))","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.713710Z","iopub.status.busy":"2024-04-28T18:47:11.713380Z","iopub.status.idle":"2024-04-28T18:47:11.717022Z","shell.execute_reply":"2024-04-28T18:47:11.716414Z"},"papermill":{"duration":0.031039,"end_time":"2024-04-28T18:47:11.718715","exception":false,"start_time":"2024-04-28T18:47:11.687676","status":"completed"},"tags":[]},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Transformer Model\n# Train Note: Use 2 GPU to get fast train\ntrainTransformer(train_dataSet, num_epochs=7, batch_size=2, parallel=True)\n# Train Note: Use CPU\n#trainTransformer(train_dataSet, num_epochs=7, batch_size=1, parallel=False)\n\ntestTransformer(_transformerModelPath, test_dataSet)","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.775902Z","iopub.status.busy":"2024-04-28T18:47:11.775005Z","iopub.status.idle":"2024-04-28T18:47:11.778859Z","shell.execute_reply":"2024-04-28T18:47:11.778153Z"},"papermill":{"duration":0.037009,"end_time":"2024-04-28T18:47:11.780641","exception":false,"start_time":"2024-04-28T18:47:11.743632","status":"completed"},"tags":[]},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Prepare Data For U-NET\n# CreateAllMasks(_trueMasksPath)\n\nframeType = 'ED'\nunet_dataset_train = getImageAndMasks(frameType=frameType, split='Train', trueMasksPath=_trueMasksPath)\nunet_dataset_test = getImageAndMasks(frameType=frameType, split='Test', trueMasksPath=_trueMasksPath)\nunet_dataset_val = getImageAndMasks(frameType=frameType, split='Val', trueMasksPath=_trueMasksPath)\n\n# ED U-NET Model\ntrainUnet(unet_dataset_train, epochs=1, batchSize=32, modelPath='', name=f'{frameType}_U_NET_Model')\ntestUnet(unet_dataset_test, path=_ED_Model_Path)","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.838040Z","iopub.status.busy":"2024-04-28T18:47:11.837681Z","iopub.status.idle":"2024-04-28T18:47:11.842265Z","shell.execute_reply":"2024-04-28T18:47:11.841118Z"},"papermill":{"duration":0.036905,"end_time":"2024-04-28T18:47:11.844255","exception":false,"start_time":"2024-04-28T18:47:11.807350","status":"completed"},"tags":[]},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"frameType = 'ES'\nunet_dataset_train = getImageAndMasks(frameType=frameType, split='Train', trueMasksPath=_trueMasksPath)\nunet_dataset_test = getImageAndMasks(frameType=frameType, split='Test', trueMasksPath=_trueMasksPath)\nunet_dataset_val = getImageAndMasks(frameType=frameType, split='Val', trueMasksPath=_trueMasksPath)\n\n# 80,20 %\nN = 550\nunet_dataset_train = unet_dataset_train.concatenate(unet_dataset_val.take(N))\nunet_dataset_val = unet_dataset_val.skip(N)\nunet_dataset_test = unet_dataset_test.concatenate(unet_dataset_val)\nprint(len(unet_dataset_train))\nprint(len(unet_dataset_test))\n\n# ES U-NET Model\ntrainUnet(unet_dataset_train, epochs=5, batchSize=32, modelPath='', name=f'{frameType}_U_NET_Model')\ntestUnet(unet_dataset_test, path=_ES_Model_Path)","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:11.902872Z","iopub.status.busy":"2024-04-28T18:47:11.902507Z","iopub.status.idle":"2024-04-28T18:47:11.906595Z","shell.execute_reply":"2024-04-28T18:47:11.905911Z"},"papermill":{"duration":0.034053,"end_time":"2024-04-28T18:47:11.908256","exception":false,"start_time":"2024-04-28T18:47:11.874203","status":"completed"},"tags":[]},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# MAIN","metadata":{"papermill":{"duration":0.026884,"end_time":"2024-04-28T18:47:11.961246","exception":false,"start_time":"2024-04-28T18:47:11.934362","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Data\ntest_dataSet = load_or_get_data('TEST')\nprint('TEST =', len(test_dataSet))","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2024-04-28T18:47:12.017583Z","iopub.status.busy":"2024-04-28T18:47:12.017179Z","iopub.status.idle":"2024-04-28T18:47:12.800208Z","shell.execute_reply":"2024-04-28T18:47:12.799457Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.812727,"end_time":"2024-04-28T18:47:12.802162","exception":false,"start_time":"2024-04-28T18:47:11.989435","status":"completed"},"tags":[]},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"TEST = 1264\n"}]},{"cell_type":"code","source":"# Load Model\ntransformerModel = loadTransformerModel(_transformerModelPath)\nED_Model = loadUnetModel(_ED_Model_Path)\nES_Model = loadUnetModel(_ES_Model_Path)","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:12.853245Z","iopub.status.busy":"2024-04-28T18:47:12.852972Z","iopub.status.idle":"2024-04-28T18:47:45.478748Z","shell.execute_reply":"2024-04-28T18:47:45.477993Z"},"papermill":{"duration":32.653507,"end_time":"2024-04-28T18:47:45.480916","exception":false,"start_time":"2024-04-28T18:47:12.827409","status":"completed"},"tags":[]},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"listEFTrue = []\nlistEFPred = []\nlistEDV = []\nlistESV = []\ni = 0\nfor obj in test_dataSet[0:]:\n    i += 1\n    print(i, obj.fileName)\n    \n    # Transformer\n    name = obj.fileName + '.avi'\n    videoPath = os.path.join(_videosPath, name)\n    ES_Frame_IMG, ED_Frame_IMG = Detect_ESED_Frame(videoPath, transformerModel)\n\n    # Mask\n    ED_true_mask = _createBinaryMask(obj.ED_landMark)\n    ES_true_mask = _createBinaryMask(obj.ES_landMark)\n\n    ED_pred_mask = predictMask(ED_Model, np.expand_dims(ED_Frame_IMG, axis=0))\n    ES_pred_mask = predictMask(ES_Model, np.expand_dims(ES_Frame_IMG, axis=0))\n\n    # Volume\n    ED_true_volume, ED_true_landmarks = get_LV_volume(ED_true_mask)\n    ES_true_volume, ES_true_landmarks = get_LV_volume(ES_true_mask)\n\n    ED_pred_volume, ED_pred_landmarks = get_LV_volume(ED_pred_mask)\n    ES_pred_volume, ES_pred_landmarks = get_LV_volume(ES_pred_mask)\n\n    # EF\n    ef_true = calculate_EF(ED_true_volume, ES_true_volume)\n    ef_pred = calculate_EF(ED_pred_volume, ES_pred_volume)\n\n    # Print\n    diffTrue = abs(obj.EF_value - ef_true)\n    diffPred = abs(obj.EF_value - ef_pred)\n\n#     print('ED_volume:', ED_pred_volume, ' ,True:', ED_true_volume)\n#     print('ES_volume:', ES_pred_volume, 'True:',   ES_true_volume)\n#     print('EF Pred:', ef_pred, 'TRUE', obj.EF_value)\n#     print('Diff True: ', diffTrue)\n#     print('Diff pred: ', diffPred)\n\n    listEFTrue.append(diffTrue)\n    listEFPred.append(diffPred)\n    listEDV.append(abs(ED_true_volume - ED_pred_volume))\n    listESV.append(abs(ES_true_volume - ES_pred_volume))\n    print('------------------------------------------------')","metadata":{"execution":{"iopub.execute_input":"2024-04-28T18:47:45.532159Z","iopub.status.busy":"2024-04-28T18:47:45.531846Z","iopub.status.idle":"2024-04-28T19:26:19.661471Z","shell.execute_reply":"2024-04-28T19:26:19.660606Z"},"papermill":{"duration":2314.157015,"end_time":"2024-04-28T19:26:19.663321","exception":false,"start_time":"2024-04-28T18:47:45.506306","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Average Error True:\", sum(listEFTrue) / len(listEFTrue))\nprint(\"Average Error Pred:\", sum(listEFPred) / len(listEFPred))\nprint(\"Average Error ED V:\", sum(listEDV) / len(listEDV))\nprint(\"Average Error ES V:\", sum(listESV) / len(listESV))\nprint(max(listEFPred))","metadata":{"execution":{"iopub.execute_input":"2024-04-28T19:26:20.406921Z","iopub.status.busy":"2024-04-28T19:26:20.406051Z","iopub.status.idle":"2024-04-28T19:26:20.412132Z","shell.execute_reply":"2024-04-28T19:26:20.411467Z"},"papermill":{"duration":0.355614,"end_time":"2024-04-28T19:26:20.413745","exception":false,"start_time":"2024-04-28T19:26:20.058131","status":"completed"},"tags":[]},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Average Error True: 1.108213933895578\n\nAverage Error Pred: 7.75653154501144\n\nAverage Error ED V: 4960.576871703875\n\nAverage Error ES V: 3037.811771074821\n\n60.44301651708293\n"}]}]}