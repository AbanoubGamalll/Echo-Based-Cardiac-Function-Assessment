{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import concatenate, Dropout, Conv2DTranspose, Input, Conv2D, MaxPooling2D\n",
    "from keras.src.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DataSet\n",
    "_dataRootPath = ''\n",
    "_videosPath = _dataRootPath + 'Videos'\n",
    "_fileNamesPath = _dataRootPath + 'FileList.csv'\n",
    "_volumeTracingPath = _dataRootPath + 'VolumeTracings.csv'\n",
    "\n",
    "# Loaded Videos\n",
    "_loadedVideosPath = ''\n",
    "\n",
    "# Transformer\n",
    "_transformerModelPath = ''\n",
    "\n",
    "# U-NET\n",
    "_trueMasksPath = ''\n",
    "_ED_Model_Path = ''\n",
    "_ES_Model_Path = ''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LandMarks:\n",
    "    def __init__(self, X1, Y1, X2, Y2):\n",
    "        self.X1 = X1\n",
    "        self.Y1 = Y1\n",
    "        self.X2 = X2\n",
    "        self.Y2 = Y2\n",
    "\n",
    "    def displayInfo(self):\n",
    "        print(f\"\"\"\n",
    "              land Marks are :\n",
    "                    X1 is  {self.X1}\n",
    "                    Y1 is {self.Y1}\n",
    "                    X2 is {self.X2}\n",
    "                    Y2 is {self.Y2}\"\"\")\n",
    "\n",
    "\n",
    "class VideoData:\n",
    "    def __init__(self, fileName, EF_value, ED_value, ES_value, ED_frame, ES_frame, Split, ED_landMark, ES_landMark,\n",
    "                 numberOfFrames,\n",
    "                 ED_Frame_IMG, ES_Frame_IMG):\n",
    "        self.fileName = fileName\n",
    "        self.EF_value = EF_value\n",
    "        self.ED_value = ED_value\n",
    "        self.ES_value = ES_value\n",
    "        self.ED_frame = ED_frame\n",
    "        self.ES_frame = ES_frame\n",
    "        self.Split = Split\n",
    "        self.ED_landMark = ED_landMark\n",
    "        self.ES_landMark = ES_landMark\n",
    "        self.numberOfFrames = numberOfFrames\n",
    "        self.ED_Frame_IMG = ED_Frame_IMG\n",
    "        self.ES_Frame_IMG = ES_Frame_IMG\n",
    "\n",
    "    def displayInfo(self):\n",
    "        print(f\"\"\"\n",
    "        Video Information:\n",
    "              File Name is  {self.fileName}\n",
    "              EF Value is {self.EF_value}\n",
    "              ES Value is {self.ES_value}\n",
    "              ED Value is {self.ED_value}\n",
    "              ED Frame is {self.ED_frame}\n",
    "              ES Frame is {self.ES_frame}\n",
    "              Split is {self.Split}\n",
    "              numberOfFrames is {self.numberOfFrames}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HelperFunction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _FilterNot_42rows(VolumeTracings, FileList):\n",
    "    VolumeTracings.dropna(inplace=True)\n",
    "    FileList.dropna(inplace=True)\n",
    "    VolumeTracings_names = VolumeTracings['FileName']\n",
    "    # VolumeTracings_names_no_extension = np.array([name[:-4] for name in VolumeTracings_names])\n",
    "\n",
    "    VolumeUniqueName, frame_counter = np.unique(VolumeTracings_names, return_counts=True)\n",
    "    Video_counts = dict(zip(VolumeUniqueName, frame_counter))\n",
    "\n",
    "    no_rows = 0\n",
    "    not_42_Rows_video_names = []\n",
    "    for vName, count in Video_counts.items():\n",
    "        if count != 42:\n",
    "            no_rows += count\n",
    "            not_42_Rows_video_names.append(vName)\n",
    "\n",
    "    VolumeTracings = VolumeTracings[~VolumeTracings['FileName'].isin(not_42_Rows_video_names)]\n",
    "    FileList = FileList[(FileList['FileName'] + \".avi\").isin(VolumeTracings['FileName'])]\n",
    "\n",
    "    # Delete rows where 'FileName' column has value '0X4F8859C8AB4DA9CB.avi'\n",
    "    VolumeTracings = VolumeTracings[VolumeTracings['FileName'] != '0X4F8859C8AB4DA9CB.avi']\n",
    "\n",
    "    return VolumeTracings, FileList\n",
    "\n",
    "\n",
    "def _loadAlldata(split_type):\n",
    "    FileList = pd.read_csv(_fileNamesPath)\n",
    "    VolumeTracings = pd.read_csv(_volumeTracingPath)\n",
    "\n",
    "    VolumeTracings, FileList = _FilterNot_42rows(VolumeTracings, FileList)\n",
    "\n",
    "    leftVentricle_list = []\n",
    "\n",
    "    VolumeTracings.dropna(inplace=True)\n",
    "    FileList.dropna(inplace=True)\n",
    "\n",
    "    for i in range(FileList.iloc[:, 0].size):\n",
    "\n",
    "        Split = FileList.iloc[i, 8]\n",
    "        if split_type != \"ALL\":\n",
    "            if split_type != Split:\n",
    "                continue\n",
    "        fileName = FileList.iloc[i, 0]\n",
    "\n",
    "        VT = VolumeTracings[VolumeTracings['FileName'] == fileName + '.avi']\n",
    "        unique_Frames = VT['Frame'].unique()\n",
    "\n",
    "        if len(unique_Frames) == 0:\n",
    "            continue\n",
    "\n",
    "        ED_Frame = unique_Frames[0]\n",
    "\n",
    "        ES_Frame = unique_Frames[1]\n",
    "        ED_tmp = VT[VT['Frame'] == ED_Frame]\n",
    "        ES_tmp = VT[VT['Frame'] == ES_Frame]\n",
    "\n",
    "        if len(ED_tmp) != 21 or len(ES_tmp) != 21:\n",
    "            continue\n",
    "        ED_landMark = LandMarks([], [], [], [])\n",
    "        ES_landMark = LandMarks([], [], [], [])\n",
    "\n",
    "        for k in range(21):\n",
    "            ED_landMark.X1.append(ED_tmp.iloc[k, 1])\n",
    "            ED_landMark.Y1.append(ED_tmp.iloc[k, 2])\n",
    "            ED_landMark.X2.append(ED_tmp.iloc[k, 3])\n",
    "            ED_landMark.Y2.append(ED_tmp.iloc[k, 4])\n",
    "\n",
    "            ES_landMark.X1.append(ES_tmp.iloc[k, 1])\n",
    "            ES_landMark.Y1.append(ES_tmp.iloc[k, 2])\n",
    "            ES_landMark.X2.append(ES_tmp.iloc[k, 3])\n",
    "            ES_landMark.Y2.append(ES_tmp.iloc[k, 4])\n",
    "\n",
    "        EF_value = FileList.iloc[i, 1]\n",
    "        ED_value = FileList.iloc[i, 2]\n",
    "        ES_value = FileList.iloc[i, 3]\n",
    "        numberOfFrames = FileList.iloc[i, 7]\n",
    "\n",
    "        video_path = os.path.join(_videosPath, fileName + '.avi')\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video file\")\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, ED_Frame - 1)\n",
    "        _, ED_Frame_IMG = cap.read()\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, ES_Frame - 1)\n",
    "        _, ES_Frame_IMG = cap.read()\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        obj = VideoData(fileName, EF_value, ED_value, ES_value, ED_Frame, ES_Frame, Split, ED_landMark,\n",
    "                        ES_landMark, numberOfFrames, ED_Frame_IMG, ES_Frame_IMG)\n",
    "\n",
    "        leftVentricle_list.append(obj)\n",
    "    return leftVentricle_list\n",
    "\n",
    "\n",
    "def load_or_get_data(spilt_type=\"ALL\"):\n",
    "    if spilt_type not in ['TRAIN', 'TEST', 'VAL', 'ALL']:\n",
    "        print('Error not valid split type')\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(_loadedVideosPath):\n",
    "        os.makedirs(_loadedVideosPath)\n",
    "        print(f'{_loadedVideosPath} created')\n",
    "\n",
    "    file_path = f'{_loadedVideosPath}/Loaded_Videos_Objects_{spilt_type}.pkl'\n",
    "    # If file exists, load the data from the file\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    # If file doesn't exist, execute loadAlldata() to get the data\n",
    "    else:\n",
    "        data = _loadAlldata(spilt_type)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _extractVideoFrames(path):\n",
    "    capture = cv2.VideoCapture(str(path))\n",
    "\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frames = np.zeros((frame_count, frame_width, frame_height, 3), np.uint8)\n",
    "\n",
    "    for count in range(frame_count):\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            raise ValueError(\"Failed to load frame #{} of {}.\".format(count, path))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames[count] = frame\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "# Transformer Data\n",
    "def _mirroringVideo(video_obj):\n",
    "    original_tuple = []\n",
    "    desired_length = 128\n",
    "\n",
    "    path = os.path.join(_videosPath, video_obj.fileName + '.avi')\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "    v = _extractVideoFrames(path)\n",
    "\n",
    "    # Mirror\n",
    "    start = min(video_obj.ED_frame, video_obj.ES_frame)\n",
    "    end = max(video_obj.ED_frame, video_obj.ES_frame) + 1\n",
    "\n",
    "    for i in range(start, end):\n",
    "        img = v[i]\n",
    "        if video_obj.ED_frame == i:\n",
    "            original_tuple.append((img, \"ED\"))\n",
    "        elif video_obj.ES_frame == i:\n",
    "            original_tuple.append((img, \"ES\"))\n",
    "        else:\n",
    "            original_tuple.append((img, \"Transition\"))\n",
    "\n",
    "    while len(original_tuple) < desired_length:\n",
    "\n",
    "        # Create a mirrored dictionary by reversing keys and values\n",
    "        mirrored_tuple = list(reversed(original_tuple))[1:]\n",
    "\n",
    "        # Append the mirrored list\n",
    "        original_tuple.extend(mirrored_tuple)\n",
    "\n",
    "        # If the list length exceeds desired_length, break the loop\n",
    "        if len(original_tuple) >= desired_length:\n",
    "            break\n",
    "\n",
    "        # Append the original list again\n",
    "        original_tuple.extend(original_tuple[1:])\n",
    "\n",
    "    # Trim the dictionary to desired_length if it exceeds it\n",
    "    original_tuple = original_tuple[:desired_length]\n",
    "\n",
    "    return original_tuple\n",
    "\n",
    "\n",
    "# U-Net Data\n",
    "def _prepareDataToPolygon(landmark):\n",
    "    data = []\n",
    "    for i in range(21):\n",
    "        data.append((landmark.X1[i], landmark.Y1[i]))\n",
    "\n",
    "    for i in range(21):\n",
    "        data.append((landmark.X2[i], landmark.Y2[i]))\n",
    "\n",
    "    if data[0][1] > data[21][1]:\n",
    "        tmp = data[0]\n",
    "        data[0] = data[21]\n",
    "        data[21] = tmp\n",
    "\n",
    "    if data[21][0] < data[20][0]:\n",
    "        tmp = data[21]\n",
    "        data[21] = data[20]\n",
    "        data[20] = tmp\n",
    "\n",
    "    tmp = data[22:]\n",
    "    data[22:] = tmp[::-1]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _createBinaryMask(landmark):\n",
    "    vertices = _prepareDataToPolygon(landmark)\n",
    "\n",
    "    # Create an empty black image\n",
    "    mask = np.zeros((112, 112)).astype(float)\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "    vertices = np.round(vertices)\n",
    "    pts = vertices.astype(int)\n",
    "\n",
    "    cv2.fillPoly(mask, [pts], color=(255, 255, 255))\n",
    "\n",
    "    mask[mask == 255] = 1\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _createImageAndMaskFolders(frameType, split, path):\n",
    "    image_path = path + f'/Frames_{frameType}/'\n",
    "    mask_path = path + f'/Masks_{frameType}/'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(image_path)\n",
    "        print(f'Frames_{frameType} create')\n",
    "    except OSError:\n",
    "        print(f'Frames_{frameType} is exist')\n",
    "\n",
    "    try:\n",
    "        os.makedirs(mask_path)\n",
    "        print(f'Masks_{frameType} create')\n",
    "    except OSError:\n",
    "        print(f'Masks_{frameType} is exist')\n",
    "\n",
    "    image_path += f'{split}/'\n",
    "    mask_path += f'{split}/'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(image_path)\n",
    "        print(f'Frames_{frameType}/{split} create')\n",
    "\n",
    "    except OSError:\n",
    "        print(f'Frames_{frameType}/{split} is exist')\n",
    "\n",
    "    try:\n",
    "        os.makedirs(mask_path)\n",
    "        print(f'Masks_{frameType}/{split} create')\n",
    "\n",
    "    except OSError:\n",
    "        print(f'Masks_{frameType}/{split} is exist')\n",
    "    return image_path, mask_path\n",
    "\n",
    "\n",
    "def _saveImageAndMask(frameType, split, trueMasksPath='', ):\n",
    "    data_set = load_or_get_data(split)\n",
    "    if data_set is None:\n",
    "        return\n",
    "\n",
    "    image_path, mask_path = _createImageAndMaskFolders(frameType, split, trueMasksPath)\n",
    "    img = None\n",
    "    landmarks = None\n",
    "    for obj in data_set:\n",
    "        if frameType == 'ES':\n",
    "            img = obj.ES_Frame_IMG\n",
    "            landmarks = obj.ES_landMark\n",
    "        elif frameType == 'ED':\n",
    "            img = obj.ED_Frame_IMG\n",
    "            landmarks = obj.ED_landMark\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = _createBinaryMask(landmarks)\n",
    "\n",
    "        cv2.imwrite(mask_path + f'{obj.fileName}.png', mask)\n",
    "        cv2.imwrite(image_path + f'{obj.fileName}.png', img)\n",
    "\n",
    "\n",
    "def CreateAllMasks(trueMasksPath):\n",
    "    _saveImageAndMask(frameType='ES', split='TRAIN', trueMasksPath=trueMasksPath)\n",
    "    _saveImageAndMask(frameType='ES', split='TEST', trueMasksPath=trueMasksPath)\n",
    "    _saveImageAndMask(frameType='ES', split='VAL', trueMasksPath=trueMasksPath)\n",
    "\n",
    "    _saveImageAndMask(frameType='ED', split='TRAIN', trueMasksPath=trueMasksPath)\n",
    "    _saveImageAndMask(frameType='ED', split='TEST', trueMasksPath=trueMasksPath)\n",
    "    _saveImageAndMask(frameType='ED', split='VAL', trueMasksPath=trueMasksPath)\n",
    "\n",
    "\n",
    "# Read Data\n",
    "def _process_path(image_path, mask_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def getImageAndMasks(frameType='', split='', trueMasksPath=''):\n",
    "    if not os.path.exists(trueMasksPath):\n",
    "        os.makedirs(trueMasksPath)\n",
    "        print(f'{trueMasksPath} created')\n",
    "    image_path = os.path.join(trueMasksPath, f\"Frames_{frameType}/{split}/\")\n",
    "    mask_path = os.path.join(trueMasksPath, f\"Masks_{frameType}/{split}/\")\n",
    "    image_list = os.listdir(image_path)\n",
    "    mask_list = os.listdir(mask_path)\n",
    "    image_list = [image_path + i for i in image_list]\n",
    "    mask_list = [mask_path + i for i in mask_list]\n",
    "\n",
    "    image_filenames = tf.constant(image_list)\n",
    "    masks_filenames = tf.constant(mask_list)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_filenames, masks_filenames))\n",
    "    print(split, len(mask_list))\n",
    "\n",
    "    image_ds = dataset.map(_process_path)\n",
    "\n",
    "    return image_ds\n",
    "\n",
    "\n",
    "# Predict LandMask From Mask\n",
    "def _getHorizontalLabel(mask):\n",
    "    upXY = (112, 0)\n",
    "\n",
    "    downXY_right = (0, 0)\n",
    "\n",
    "    downXY_left = (0, 112)\n",
    "\n",
    "    for x in range(112):\n",
    "        for y in range(112):\n",
    "            if mask[x][y] == 0:\n",
    "                continue\n",
    "\n",
    "            if upXY[0] > x and upXY[1] < y:\n",
    "                upXY = (x, y)\n",
    "            elif downXY_right[0] <= x and downXY_right[1] <= y:\n",
    "                downXY_right = (x, y)\n",
    "            elif downXY_left[0] < x or downXY_left[1] > y:\n",
    "                downXY_left = (x, y)\n",
    "\n",
    "    midpoint = ((downXY_left[0] + downXY_right[0]) // 2, (downXY_left[1] + downXY_right[1]) // 2)\n",
    "    for x in range(112):\n",
    "        for y in range(112):\n",
    "            midpoint = (midpoint[0] + 1, midpoint[1])\n",
    "\n",
    "            if midpoint[0] == 112 or mask[midpoint[0]][midpoint[1]] == 0:\n",
    "                midpoint = (midpoint[0] - 1, midpoint[1])\n",
    "                break\n",
    "\n",
    "    #     for i in range(112):\n",
    "    #         for j in range(112):\n",
    "    #             if mask[i][j] == 1:\n",
    "    #                 img[i][j] = (250, 250, 250)\n",
    "\n",
    "    # plt.scatter(upXY[1], upXY[0], color='orange', marker='o')\n",
    "    #\n",
    "    # plt.scatter(downXY_right[1], downXY_right[0], color='orange', marker='o')\n",
    "    #\n",
    "    # plt.scatter(downXY_left[1], downXY_left[0], color='orange', marker='o')\n",
    "    # plt.scatter(midpoint[1], midpoint[0], color='orange', marker='X')\n",
    "\n",
    "    if midpoint[1] == upXY[1]:\n",
    "        midpoint = (midpoint[0], midpoint[1] + 0.1)\n",
    "\n",
    "    return (midpoint[1], midpoint[0]), (upXY[1], upXY[0])\n",
    "\n",
    "\n",
    "def _perpendicular_points(x1, y1, x2, y2, distance):\n",
    "    # Calculate slope of the original line\n",
    "    if x2 - x1 != 0:  # Avoid division by zero\n",
    "        slope_original = (y2 - y1) / (x2 - x1)\n",
    "        # Calculate negative reciprocal to get slope of perpendicular line\n",
    "        slope_perpendicular = -1 / slope_original\n",
    "    else:\n",
    "        slope_perpendicular = float('inf')  # Handle vertical lines\n",
    "\n",
    "    # Find midpoint of the original line\n",
    "\n",
    "    # Calculate unit vector along the original line\n",
    "    magnitude = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "    unit_vector_x = (x2 - x1) / magnitude\n",
    "    unit_vector_y = (y2 - y1) / magnitude\n",
    "\n",
    "    # Calculate displacement vector based on distance\n",
    "    displacement_x = unit_vector_x * distance\n",
    "    displacement_y = unit_vector_y * distance\n",
    "\n",
    "    # New midpoint for the perpendicular line\n",
    "    new_mid_x = x1 + displacement_x\n",
    "    new_mid_y = y1 + displacement_y\n",
    "\n",
    "    # Find points for the perpendicular line\n",
    "    dx = 1 / (1 + slope_perpendicular ** 2) ** 0.5\n",
    "    dy = slope_perpendicular * dx\n",
    "\n",
    "    # Two points for the perpendicular line\n",
    "    perpendicular_point1 = (new_mid_x + dx, new_mid_y + dy)\n",
    "    perpendicular_point2 = (new_mid_x - dx, new_mid_y - dy)\n",
    "\n",
    "    return perpendicular_point1, perpendicular_point2\n",
    "\n",
    "\n",
    "def _find_previous_or_next_points(point1=None, point2=None, t='n'):\n",
    "    # Extract coordinates of the two points\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    x, y = 0, 0\n",
    "    # Calculate the distance between point1 and point2\n",
    "    distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "\n",
    "    # Calculate the slope of the line\n",
    "    if x2 - x1 != 0:  # Avoid division by zero\n",
    "        slope = (y2 - y1) / (x2 - x1)\n",
    "    else:\n",
    "        slope = None  # Line is vertical\n",
    "\n",
    "    if t == 'n':\n",
    "        # Calculate the next point\n",
    "        if slope is not None:\n",
    "            # If the line is not vertical, find next x and y\n",
    "            x = x2 + (x2 - x1) / distance\n",
    "            y = y2 + (y2 - y1) / distance\n",
    "        else:\n",
    "            # If the line is vertical, next point has the same x-coordinate\n",
    "            x = x2\n",
    "            y = y2 - distance\n",
    "    if t == 'p':\n",
    "        # Calculate the previous point\n",
    "        if slope is not None:\n",
    "            # If the line is not vertical, find previous x and y\n",
    "            x = x1 - (x2 - x1) / distance\n",
    "            y = y1 - (y2 - y1) / distance\n",
    "        else:\n",
    "            # If the line is vertical, previous point has the same x-coordinate\n",
    "            x = x1\n",
    "            y = y1 - distance\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def _getPointsInMask(point1, point2, mask):\n",
    "    # plt.plot([point1[0], point2[0]], [point1[1], point2[1]], marker='o', label='Points and Line')\n",
    "\n",
    "    pn = point2\n",
    "    lpn = None\n",
    "    for i in range(112):\n",
    "        if i == 0:\n",
    "            pn = _find_previous_or_next_points(point1, point2, t='n')\n",
    "            lpn = pn\n",
    "        x = int(np.round(pn[1]))\n",
    "        y = int(np.round(pn[0]))\n",
    "        if x == 112 or mask[x][y] == 0:\n",
    "            break\n",
    "        else:\n",
    "            lpn = pn\n",
    "            pn = _find_previous_or_next_points(point1, pn, t='n')\n",
    "        # plt.plot(pn[0], pn[1], marker='o', color='red', label='Next Point')\n",
    "\n",
    "    pn = lpn\n",
    "\n",
    "    pp = None\n",
    "    lpp = point1\n",
    "    for i in range(112):\n",
    "        if i == 0:\n",
    "            pp = _find_previous_or_next_points(point1, point2, t='p')\n",
    "        if int(np.round(pp[1])) == 112 or int(np.round(pp[0])) == 112 \\\n",
    "                or mask[int(np.round(pp[1]))][int(np.round(pp[0]))] == 0:\n",
    "            break\n",
    "        else:\n",
    "            lpp = pp\n",
    "            pp = _find_previous_or_next_points(pp, point2, t='p')\n",
    "        # plt.plot(pp[0], pp[1], marker='o', color='red')\n",
    "\n",
    "    pp = lpp\n",
    "\n",
    "    # plt.plot([pp[0], pn[0]], [pp[1], pn[1]], color='blue')\n",
    "\n",
    "    return pn, pp\n",
    "\n",
    "\n",
    "def _GetPointOfSegmentMask(mask=None):\n",
    "    labels = [_getHorizontalLabel(mask)]\n",
    "\n",
    "    x1, y1 = labels[0][0]\n",
    "    x2, y2 = labels[0][1]\n",
    "\n",
    "    numOfLines = 20\n",
    "    distance = math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2) / numOfLines\n",
    "\n",
    "    point = []\n",
    "    for i in range(numOfLines):\n",
    "        point1, point2 = _perpendicular_points(x1, y1, x2, y2, distance * i)\n",
    "        point.append((point1, point2))\n",
    "\n",
    "        # plt.plot([x1, x2], [y1, y2], label=\"Original Line\")\n",
    "        # plt.plot([point1[0], point2[0]], [point1[1], point2[1]], label=\"Perpendicular Line\")\n",
    "        # plt.scatter([x1, x2], [y1, y2], color='red')\n",
    "        # plt.scatter([point1[0], point2[0]], [point1[1], point2[1]], color='blue')\n",
    "\n",
    "    for i in range(numOfLines):\n",
    "        labels.append(_getPointsInMask(point[i][0], point[i][1], mask))\n",
    "\n",
    "    #     for i in range(112):\n",
    "    #         for j in range(112):\n",
    "    #             if mask[i][j] == 1:\n",
    "    #                 img[i][j] = (250, 250, 250)\n",
    "\n",
    "    #     x = 'x'\n",
    "    #     for p1, p2 in labels:\n",
    "    #         plt.plot(p1[0], p1[1], marker=x, color='red', label='Next Point')\n",
    "    #         plt.plot(p2[0], p2[1], marker=x, color='red', label='Next Point')\n",
    "    #         plt.plot([p1[0], p2[0]], [p1[1], p2[1]], color='blue')\n",
    "    #         x = 'o'\n",
    "\n",
    "    landmarks_pred = LandMarks([], [], [], [])\n",
    "\n",
    "    for i in range(21):\n",
    "        (x1, y1), (x2, y2) = labels[i]\n",
    "        landmarks_pred.X1.append(x1)\n",
    "        landmarks_pred.Y1.append(y1)\n",
    "\n",
    "        landmarks_pred.X2.append(x2)\n",
    "        landmarks_pred.Y2.append(y2)\n",
    "\n",
    "    # Arrange landmarks like Dataset\n",
    "    landmarks_pred.X1[0], landmarks_pred.X2[0] = landmarks_pred.X2[0], landmarks_pred.X1[0]\n",
    "    landmarks_pred.Y1[0], landmarks_pred.Y2[0] = landmarks_pred.Y2[0], landmarks_pred.Y1[0]\n",
    "\n",
    "    landmarks_pred.X1[1:21] = landmarks_pred.X1[1:21][::-1]\n",
    "    landmarks_pred.Y1[1:21] = landmarks_pred.Y1[1:21][::-1]\n",
    "\n",
    "    landmarks_pred.X2[1:21] = landmarks_pred.X2[1:21][::-1]\n",
    "    landmarks_pred.Y2[1:21] = landmarks_pred.Y2[1:21][::-1]\n",
    "\n",
    "    return landmarks_pred\n",
    "\n",
    "\n",
    "# Calc Volume and EF\n",
    "def _calculate_volume(landmarks):\n",
    "    X1 = landmarks.X1\n",
    "    Y1 = landmarks.Y1\n",
    "    X2 = landmarks.X2\n",
    "    Y2 = landmarks.Y2\n",
    "    verticalLine_distance = math.sqrt((X2[0] - X1[0]) ** 2 + (Y2[0] - Y1[0]) ** 2)\n",
    "    dx = verticalLine_distance / 20\n",
    "\n",
    "    volume = 0\n",
    "    for i in range(1, 21):\n",
    "        volume += (math.pi * ((X2[i] - X1[i]) ** 2 + (Y2[i] - Y1[i]) ** 2) * dx) / 4.0\n",
    "\n",
    "    return volume\n",
    "\n",
    "\n",
    "def calculate_EF(ED_volume, ES_volume):\n",
    "    return (abs(abs(ED_volume) - abs(ES_volume)) / ED_volume) * 100\n",
    "\n",
    "\n",
    "def get_LV_volume(mask):\n",
    "    landmarks = _GetPointOfSegmentMask(mask)\n",
    "    volume = _calculate_volume(landmarks)\n",
    "    return volume, landmarks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataSet Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VideoDataSetForModel(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataSet=None, fullVideo=False):\n",
    "\n",
    "        self.dataSet = dataSet\n",
    "        self.fullVideo = fullVideo\n",
    "        self.frame_width = self.frame_height = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataSet)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        obj = self.dataSet[index]\n",
    "\n",
    "        if self.fullVideo:\n",
    "            frame_count = obj.numberOfFrames\n",
    "            path = os.path.join(_videosPath, obj.fileName + '.avi')\n",
    "            video = _extractVideoFrames(path)\n",
    "        else:\n",
    "            frame_count = 128\n",
    "            video = _mirroringVideo(obj)\n",
    "\n",
    "        frames = np.zeros((frame_count, 112, 112, 3), np.float32)\n",
    "        labels = np.zeros(frame_count, np.int8)\n",
    "\n",
    "        if self.fullVideo:\n",
    "            frames = video\n",
    "            labels[obj.ES_frame] = 1\n",
    "            labels[obj.ED_frame] = 2\n",
    "        else:\n",
    "            for i in range(0, frame_count):\n",
    "                # 0 TR , 1 ES, 2 ED\n",
    "                label = video[i][1]\n",
    "                if label == 'ES':\n",
    "                    label = 1\n",
    "                elif label == 'ED':\n",
    "                    label = 2\n",
    "                else:\n",
    "                    label = 0\n",
    "\n",
    "                frames[i] = video[i][0]\n",
    "                labels[i] = label\n",
    "\n",
    "        # (F,W,H,C) > F C W H\n",
    "        frames = frames.transpose((3, 0, 1, 2))\n",
    "\n",
    "        ########################\n",
    "        # Load video into np.array\n",
    "        frames = frames.astype(np.float32)\n",
    "\n",
    "        # Scale pixel values from 0-255 to 0-1\n",
    "        frames /= 255.0\n",
    "\n",
    "        frames = np.moveaxis(frames, 0, 1)\n",
    "        p = 8\n",
    "        frames = np.pad(frames, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant', constant_values=0)\n",
    "        ########################\n",
    "\n",
    "        return frames, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1)):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.residual_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                            padding=1),\n",
    "            torch.nn.BatchNorm2d(in_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                            padding=1),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.residual_block(x)\n",
    "\n",
    "\n",
    "class ResNetEncoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_ResidualBlock=8,\n",
    "                 n_levels=4,\n",
    "                 input_ch=3,\n",
    "                 z_dim=10,\n",
    "                 bUseMultiResSkips=True):\n",
    "\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "\n",
    "        self.max_filters = 2 ** (n_levels + 3)\n",
    "        self.n_levels = n_levels\n",
    "        self.bUseMultiResSkips = bUseMultiResSkips\n",
    "\n",
    "        self.conv_list = torch.nn.ModuleList()\n",
    "        self.res_blk_list = torch.nn.ModuleList()\n",
    "        self.multi_res_skip_list = torch.nn.ModuleList()\n",
    "\n",
    "        self.input_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=input_ch, out_channels=8,\n",
    "                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        for i in range(n_levels):\n",
    "            n_filters_1 = 2 ** (i + 3)\n",
    "            n_filters_2 = 2 ** (i + 4)\n",
    "            ks = 2 ** (n_levels - i)\n",
    "\n",
    "            self.res_blk_list.append(\n",
    "                torch.nn.Sequential(*[ResidualBlock(n_filters_1, n_filters_1)\n",
    "                                      for _ in range(n_ResidualBlock)])\n",
    "            )\n",
    "\n",
    "            self.conv_list.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(n_filters_1, n_filters_2,\n",
    "                                    kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
    "                    torch.nn.BatchNorm2d(n_filters_2),\n",
    "                    torch.nn.ReLU(inplace=True),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if bUseMultiResSkips:\n",
    "                self.multi_res_skip_list.append(\n",
    "                    torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(in_channels=n_filters_1, out_channels=self.max_filters, kernel_size=(ks, ks),\n",
    "                                        stride=(ks, ks), padding=0),\n",
    "                        torch.nn.BatchNorm2d(self.max_filters),\n",
    "                        torch.nn.ReLU(inplace=True),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.output_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=self.max_filters, out_channels=z_dim,\n",
    "                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "            torch.nn.BatchNorm2d(z_dim),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        skips = []\n",
    "        for i in range(self.n_levels):\n",
    "            x = self.res_blk_list[i](x)\n",
    "            if self.bUseMultiResSkips:\n",
    "                skips.append(self.multi_res_skip_list[i](x))\n",
    "            x = self.conv_list[i](x)\n",
    "\n",
    "        if self.bUseMultiResSkips:\n",
    "            x = sum([x] + skips)\n",
    "\n",
    "        x = self.output_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetDecoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_ResidualBlock=8,\n",
    "                 n_levels=4,\n",
    "                 z_dim=10,\n",
    "                 output_channels=3,\n",
    "                 bUseMultiResSkips=True):\n",
    "\n",
    "        super(ResNetDecoder, self).__init__()\n",
    "\n",
    "        self.max_filters = 2 ** (n_levels + 3)\n",
    "        self.n_levels = n_levels\n",
    "        self.bUseMultiResSkips = bUseMultiResSkips\n",
    "\n",
    "        self.conv_list = torch.nn.ModuleList()\n",
    "        self.res_blk_list = torch.nn.ModuleList()\n",
    "        self.multi_res_skip_list = torch.nn.ModuleList()\n",
    "\n",
    "        self.input_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=z_dim, out_channels=self.max_filters,\n",
    "                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "            torch.nn.BatchNorm2d(self.max_filters),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        n_filters_1 = 2 ** (self.n_levels - 0 + 2)\n",
    "        for i in range(n_levels):\n",
    "            n_filters_0 = 2 ** (self.n_levels - i + 3)\n",
    "            n_filters_1 = 2 ** (self.n_levels - i + 2)\n",
    "            ks = 2 ** (i + 1)\n",
    "\n",
    "            self.res_blk_list.append(\n",
    "                torch.nn.Sequential(*[ResidualBlock(n_filters_1, n_filters_1)\n",
    "                                      for _ in range(n_ResidualBlock)])\n",
    "            )\n",
    "\n",
    "            self.conv_list.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.ConvTranspose2d(n_filters_0, n_filters_1,\n",
    "                                             kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
    "                    torch.nn.BatchNorm2d(n_filters_1),\n",
    "                    torch.nn.ReLU(inplace=True),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if bUseMultiResSkips:\n",
    "                self.multi_res_skip_list.append(\n",
    "                    torch.nn.Sequential(\n",
    "                        torch.nn.ConvTranspose2d(in_channels=self.max_filters, out_channels=n_filters_1,\n",
    "                                                 kernel_size=(ks, ks), stride=(ks, ks), padding=0),\n",
    "                        torch.nn.BatchNorm2d(n_filters_1),\n",
    "                        torch.nn.ReLU(inplace=True),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.output_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=n_filters_1, out_channels=output_channels,\n",
    "                            kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "            # torch.nn.BatchNorm2d(output_channels),\n",
    "            # torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "\n",
    "        z = z_top = self.input_conv(z)\n",
    "\n",
    "        for i in range(self.n_levels):\n",
    "            z = self.conv_list[i](z)\n",
    "            z = self.res_blk_list[i](z)\n",
    "            if self.bUseMultiResSkips:\n",
    "                z += self.multi_res_skip_list[i](z_top)\n",
    "\n",
    "        z = self.output_conv(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class ResNetAE(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape=(256, 256, 3),\n",
    "                 n_ResidualBlock=8,\n",
    "                 n_levels=4,\n",
    "                 z_dim=128,\n",
    "                 bottleneck_dim=128,\n",
    "                 bUseMultiResSkips=True):\n",
    "        super(ResNetAE, self).__init__()\n",
    "\n",
    "        assert input_shape[0] == input_shape[1]\n",
    "        image_channels = input_shape[2]\n",
    "        self.z_dim = z_dim\n",
    "        self.img_latent_dim = input_shape[0] // (2 ** n_levels)\n",
    "\n",
    "        self.encoder = ResNetEncoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n",
    "                                     input_ch=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n",
    "        self.decoder = ResNetDecoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n",
    "                                     output_channels=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.z_dim * self.img_latent_dim * self.img_latent_dim, bottleneck_dim)\n",
    "        self.fc2 = torch.nn.Linear(bottleneck_dim, self.z_dim * self.img_latent_dim * self.img_latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return torch.tanh(self.fc1(h.view(x.shape[0], self.z_dim * self.img_latent_dim * self.img_latent_dim)))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder(self.fc2(z).view(-1, self.z_dim, self.img_latent_dim, self.img_latent_dim))\n",
    "        return torch.sigmoid(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n",
    "\n",
    "def reParameterize(mu, log):\n",
    "    std = torch.exp(0.5 * log)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "class ResNetVAE(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape=(256, 256, 3),\n",
    "                 n_ResidualBlock=8,\n",
    "                 n_levels=4,\n",
    "                 z_dim=128,\n",
    "                 bottleneck_dim=128,\n",
    "                 bUseMultiResSkips=True):\n",
    "        super(ResNetVAE, self).__init__()\n",
    "\n",
    "        assert input_shape[0] == input_shape[1]\n",
    "        image_channels = input_shape[2]\n",
    "        self.z_dim = z_dim\n",
    "        self.img_latent_dim = input_shape[0] // (2 ** n_levels)\n",
    "\n",
    "        self.encoder = ResNetEncoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n",
    "                                     input_ch=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n",
    "        self.decoder = ResNetDecoder(n_ResidualBlock=n_ResidualBlock, n_levels=n_levels,\n",
    "                                     output_channels=image_channels, z_dim=z_dim, bUseMultiResSkips=bUseMultiResSkips)\n",
    "\n",
    "        # Assumes the input to be of shape 256x256\n",
    "        self.fc21 = torch.nn.Linear(self.z_dim * self.img_latent_dim * self.img_latent_dim, bottleneck_dim)\n",
    "        self.fc22 = torch.nn.Linear(self.z_dim * self.img_latent_dim * self.img_latent_dim, bottleneck_dim)\n",
    "        self.fc3 = torch.nn.Linear(bottleneck_dim, self.z_dim * self.img_latent_dim * self.img_latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1.view(-1, self.z_dim * self.img_latent_dim * self.img_latent_dim)), \\\n",
    "            self.fc22(h1.view(-1, self.z_dim * self.img_latent_dim * self.img_latent_dim))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.decoder(self.fc3(z).view(-1, self.z_dim, self.img_latent_dim, self.img_latent_dim))\n",
    "        return torch.sigmoid(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log = self.encode(x)\n",
    "        z = reParameterize(mu, log)\n",
    "        return self.decode(z), mu, log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, num_hidden_layers=16, attention_heads=16, intermediate_size=8192,\n",
    "                 input_shape=(128, 128, 3)):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # ResNet encoder\n",
    "        self.model_AE = ResNetAE(input_shape=input_shape, n_ResidualBlock=8, n_levels=4,\n",
    "                                 bottleneck_dim=embedding_dim)\n",
    "        self.model_AE.decoder = None\n",
    "        self.model_AE.fc2 = None\n",
    "\n",
    "        # BertModel encoder\n",
    "        configuration = BertConfig(\n",
    "            vocab_size=1,  # Set to 0/None ?\n",
    "            hidden_size=embedding_dim,  # Length of embeddings\n",
    "            num_hidden_layers=num_hidden_layers,  # 16\n",
    "            num_attention_heads=attention_heads,\n",
    "            intermediate_size=intermediate_size,  # 8192\n",
    "            hidden_act='gelu',\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=1024,  # 64 ?\n",
    "            type_vocab_size=1,\n",
    "            initializer_range=0.02,\n",
    "            layer_norm_eps=1e-12,\n",
    "            pad_token_id=0,\n",
    "            gradient_checkpointing=False,\n",
    "            position_embedding_type='absolute',\n",
    "            use_cache=True)\n",
    "\n",
    "        configuration.num_labels = 3\n",
    "\n",
    "        self.model_Bert = BertModel(configuration).encoder\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        last_features = 3\n",
    "        self.extremas = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=embedding_dim // 2, bias=True),\n",
    "            nn.LayerNorm(embedding_dim // 2),\n",
    "            nn.LeakyReLU(negative_slope=0.05, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=embedding_dim // 2, out_features=embedding_dim // 4, bias=True),\n",
    "            nn.LayerNorm(embedding_dim // 4),\n",
    "            nn.LeakyReLU(negative_slope=0.05, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=embedding_dim // 4, out_features=last_features, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, frames, nF):\n",
    "        # (BxF) x C x H x W => (BxF) x Emb\n",
    "        # Frame [128, 3, 128, 128]\n",
    "        frames = self.model_AE.encode(frames).squeeze()\n",
    "        # embeddings [128, 1024]\n",
    "        # output_bert\n",
    "        # B x F x Emb => AttHeads+1 x B x F x Emb\n",
    "        frames = self.model_Bert(frames.view(-1, nF, self.embedding_dim), output_hidden_states=True)\n",
    "        # AttHeads+1 x B x F x Emb => B x F x Emb\n",
    "        frames = torch.stack(frames.hidden_states).mean(dim=0)\n",
    "        # B x F x Emb => B x F x 1\n",
    "        frames = self.extremas(frames)\n",
    "        # classes_vec [1, 128, 3]\n",
    "        return frames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformer Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, optim, device):\n",
    "    total = 0.\n",
    "    n = 0\n",
    "    loss_hist = []\n",
    "\n",
    "    model.train(True)\n",
    "    print(\"Learning rate:\", get_lr(optim))\n",
    "\n",
    "    weighting = torch.tensor([1., 5., 5.]).to(device)\n",
    "    loss_fct1 = nn.CrossEntropyLoss(weight=weighting, reduction='mean')\n",
    "\n",
    "    with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "        for (frames, label) in dataloader:\n",
    "            nB, nF, nC, nH, nW = frames.shape\n",
    "\n",
    "            # Merge batch and frames dimension\n",
    "            frames = frames.view(nB * nF, nC, nH, nW)\n",
    "            frames = frames.to(device, dtype=torch.float32)\n",
    "\n",
    "            # (F*B) X C x W X H\n",
    "            class_vec = model(frames, nF)\n",
    "\n",
    "            label = label.to(device, dtype=torch.long)\n",
    "\n",
    "            loss1 = loss_fct1(class_vec.view(-1, 3), label.view(-1))\n",
    "            loss = loss1\n",
    "\n",
    "            # Take gradient step if training\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Accumulate losses and compute baselines\n",
    "            total += loss.item()\n",
    "            n += 1\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "            avg = np.mean(loss_hist[max(-len(loss_hist), -10):])\n",
    "\n",
    "            # Show info on process bar\n",
    "            pbar.set_postfix_str(\"{:.4f} / {:.4f} / {:.4f}\".format(total / n, loss1.item(), avg))\n",
    "            pbar.update()\n",
    "    loss_hist = np.array(loss_hist)\n",
    "\n",
    "    return (total / 1), loss_hist\n",
    "\n",
    "\n",
    "def trainTransformer(train_dataSet, num_epochs, batch_size, parallel):\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if type(device) == type(list()):\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in device)\n",
    "        device = \"cuda\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Model\n",
    "    model = TransformerModel(embedding_dim=1024, num_hidden_layers=16, attention_heads=16, intermediate_size=8192,\n",
    "                             input_shape=(128, 128, 3))\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print('model name', model.__class__.__name__, \"contains\", pytorch_total_params, \"parameters.\")\n",
    "\n",
    "    if parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # DATA SETUP\n",
    "    train_dataSet = VideoDataSetForModel(dataSet=train_dataSet, fullVideo=False)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataSet, batch_size=batch_size, num_workers=0,\n",
    "                                                   shuffle=True,\n",
    "                                                   pin_memory=(device.type == \"cuda\"),\n",
    "                                                   drop_last=True)\n",
    "\n",
    "    dataloaders = {'train': train_dataloader}\n",
    "    # len(dataloaders['train'])\n",
    "\n",
    "    # Set up optimizer\n",
    "    lr = 1e-5\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_step_period = 1\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, lr_step_period)\n",
    "\n",
    "    bestLoss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"Epoch {} / {}\".format(epoch, num_epochs), flush=True)\n",
    "        for phase in ['train']:  # , 'val']:\n",
    "            print(\"Running on\", phase)\n",
    "            loss, _ = run_epoch(model, dataloaders[phase], optim, device)\n",
    "            print('Loss =', loss)\n",
    "            print()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save checkpoint\n",
    "        save = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': bestLoss,\n",
    "            'loss': loss,\n",
    "            'opt_dict': optim.state_dict(),\n",
    "            'scheduler_dict': scheduler.state_dict(),\n",
    "        }\n",
    "        if loss < bestLoss:\n",
    "            print('new Best Version')\n",
    "            torch.save(save, \"best.pt\")\n",
    "            bestLoss = loss\n",
    "        else:\n",
    "            torch.save(save, \"checkpoint_\" + str(epoch) + \".pt\")\n",
    "\n",
    "\n",
    "# Test\n",
    "def show_graph(label, predict):\n",
    "    predict = predict.copy() + 0.5\n",
    "    plt.plot(predict, label='predict')\n",
    "    plt.plot(label, label='Label')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def smooth(vec, window=5, rep=1):\n",
    "    weight = torch.ones((1, 1, window)) / window\n",
    "    for _ in range(rep):\n",
    "        pad = int((window - 1) / 2)\n",
    "        vec = vec.unsqueeze(0).unsqueeze(0)\n",
    "        vec = torch.nn.functional.conv1d(vec, weight, bias=None, stride=1, padding=pad, dilation=1, groups=1).squeeze()\n",
    "    return vec\n",
    "\n",
    "\n",
    "def loadTransformerModel(path):\n",
    "    best = torch.load(path, map_location=\"cpu\")\n",
    "    model = TransformerModel(embedding_dim=1024, num_hidden_layers=16, attention_heads=16, intermediate_size=8192,\n",
    "                             input_shape=(128, 128, 3))\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.load_state_dict(best['state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def GetLengthOfEachBet(predict, deleteLastHalfBet=False):\n",
    "    lengthBet = []\n",
    "    firstFrames = []\n",
    "    lastFrames = []\n",
    "    check = True\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        if predict[i] != 0:\n",
    "            if check:\n",
    "                firstFrames.append(i)\n",
    "                check = False\n",
    "            else:\n",
    "                lastFrames.append(i)\n",
    "                check = True\n",
    "\n",
    "    if deleteLastHalfBet:\n",
    "        # Delete the last half Bet\n",
    "        if len(firstFrames) > len(lastFrames):\n",
    "            predict[firstFrames[-1]] = 0\n",
    "            firstFrames.pop()\n",
    "        elif len(firstFrames) < len(lastFrames):\n",
    "            predict[lastFrames[-1]] = 0\n",
    "            lastFrames.pop()\n",
    "\n",
    "    for i in range(len(firstFrames)):\n",
    "        lengthBet.append(lastFrames[i] - firstFrames[i])\n",
    "\n",
    "    return lengthBet, firstFrames, lastFrames\n",
    "\n",
    "\n",
    "def testForOneVideo(model, frames, device):\n",
    "    nB, nF, nC, nH, nW = frames.shape\n",
    "    frames = torch.cat(([frames[i] for i in range(frames.size(0))]), dim=0)\n",
    "    frames = frames.to(device, dtype=torch.float)\n",
    "\n",
    "    class_vec = model(frames, nF).squeeze()\n",
    "\n",
    "    class_diff = class_vec[:, 2] - class_vec[:, 1]\n",
    "\n",
    "    smooth_vec = smooth(class_diff, window=5, rep=3).detach().numpy()\n",
    "\n",
    "    # Get Peaks\n",
    "    predict = np.zeros((len(smooth_vec)), np.int8)\n",
    "    for i in range(len(smooth_vec)):\n",
    "        if i == 0 or i == len(smooth_vec) - 1:\n",
    "            continue\n",
    "        if smooth_vec[i] < smooth_vec[i + 1] and smooth_vec[i] < smooth_vec[i - 1]:\n",
    "            predict[i] = 1\n",
    "        if smooth_vec[i] > smooth_vec[i + 1] and smooth_vec[i] > smooth_vec[i - 1]:\n",
    "            predict[i] = 2\n",
    "\n",
    "    # Get length of each bet\n",
    "    lengthBet, firstFrames, lastFrames = GetLengthOfEachBet(predict, True)\n",
    "\n",
    "    # Apply Threshold\n",
    "    thr = max(lengthBet) * 0.35\n",
    "    for i in range(len(lengthBet)):\n",
    "        if thr > lengthBet[i]:\n",
    "            predict[firstFrames[i]] = 0\n",
    "            predict[lastFrames[i]] = 0\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def testTransformer(transformer_path, dataSet):\n",
    "    device = 'cpu'\n",
    "    device = torch.device(device)\n",
    "    model = loadTransformerModel(transformer_path)\n",
    "\n",
    "    dataSet = VideoDataSetForModel(dataSet=dataSet, fullVideo=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataSet, batch_size=1, shuffle=False)\n",
    "    trueFrames = 0\n",
    "    trueTransitionFrames = 0\n",
    "    trueESFrames = 0\n",
    "    trueEDFrames = 0\n",
    "    totalFrames = 0\n",
    "    trueFrames2 = 0\n",
    "    with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "        for frames, label in dataloader:\n",
    "            predict = testForOneVideo(model, frames, device)\n",
    "            label = label.squeeze().detach().numpy()\n",
    "\n",
    "            totalFrames += len(label)\n",
    "            for i in range(len(label)):\n",
    "                if predict[i] == label[i]:\n",
    "                    trueFrames += 1\n",
    "                if predict[i] == 0 and label[i] == 0:\n",
    "                    trueTransitionFrames += 1\n",
    "                elif predict[i] == 1 and label[i] == 1:\n",
    "                    trueEDFrames += 1\n",
    "                elif predict[i] == 2 and label[i] == 2:\n",
    "                    trueESFrames += 1\n",
    "\n",
    "                if label[i] != 0:\n",
    "                    if predict[i] == label[i]:\n",
    "                        trueFrames2 += 1\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    accuracy = (trueFrames / totalFrames) * 100\n",
    "    print('Accuracy: ', accuracy)\n",
    "\n",
    "    accuracy2 = (trueFrames2 / (2 * len(dataloader))) * 100\n",
    "    print('Accuracy ES & ED: ', accuracy2)\n",
    "\n",
    "    accuracyED = (trueEDFrames / len(dataloader)) * 100\n",
    "    print('Accuracy ED: ', accuracyED)\n",
    "\n",
    "    accuracyES = (trueESFrames / len(dataloader)) * 100\n",
    "    print('Accuracy ES: ', accuracyES)\n",
    "\n",
    "    accuracyTransition = (trueTransitionFrames / (totalFrames - len(dataloader) * 2)) * 100\n",
    "    print('Accuracy Transition: ', accuracyTransition)\n",
    "\n",
    "\n",
    "# Detect ES & ED Frame\n",
    "def Detect_ESED_Frame(video_path, transformerModel, labels=None):\n",
    "    device = 'cpu'\n",
    "    device = torch.device(device)\n",
    "    # Prepare Video\n",
    "    frames = _extractVideoFrames(video_path)\n",
    "    # (F,W,H,C) > F C W H\n",
    "    frames = frames.transpose((3, 0, 1, 2))\n",
    "    # Load video into np.array\n",
    "    frames = frames.astype(np.float32)\n",
    "    # Scale pixel values from 0-255 to 0-1\n",
    "    frames /= 255.0\n",
    "\n",
    "    frames = np.moveaxis(frames, 0, 1)\n",
    "    p = 8\n",
    "    frames = np.pad(frames, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant', constant_values=0)\n",
    "\n",
    "    frames = torch.from_numpy(frames)\n",
    "    frames = frames.unsqueeze(0)\n",
    "\n",
    "    predict = testForOneVideo(transformerModel, frames, device)\n",
    "\n",
    "    lengthBet, firstFrames, lastFrames = GetLengthOfEachBet(predict, False)\n",
    "\n",
    "    maxIDX = 0\n",
    "    for i in range(1, len(lengthBet)):\n",
    "        if lengthBet[maxIDX] <= lengthBet[i]:\n",
    "            maxIDX = i\n",
    "\n",
    "    frames = frames.squeeze()\n",
    "    if predict[firstFrames[maxIDX]] == 1:\n",
    "        ES_Frame_IMG = np.transpose(frames[firstFrames[maxIDX]], (1, 2, 0))\n",
    "        ED_Frame_IMG = np.transpose(frames[lastFrames[maxIDX]], (1, 2, 0))\n",
    "    else:\n",
    "        ED_Frame_IMG = np.transpose(frames[firstFrames[maxIDX]], (1, 2, 0))\n",
    "        ES_Frame_IMG = np.transpose(frames[lastFrames[maxIDX]], (1, 2, 0))\n",
    "\n",
    "    # Show 4 Frames\n",
    "    if labels is not None:\n",
    "        print(firstFrames[maxIDX], lastFrames[maxIDX])\n",
    "        TrueES_Frame = 0\n",
    "        TrueED_Frame = 0\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                TrueES_Frame = np.transpose(frames[i], (1, 2, 0))\n",
    "            elif labels[i] == 2:\n",
    "                TrueED_Frame = np.transpose(frames[i], (1, 2, 0))\n",
    "\n",
    "            fig, axes = plt.subplots(2, 2)\n",
    "\n",
    "            axes[0][0].imshow(ES_Frame_IMG)\n",
    "            axes[0][0].set_title('ES Pred')\n",
    "            axes[0][0].axis('off')\n",
    "\n",
    "            axes[0][1].imshow(TrueES_Frame)\n",
    "            axes[0][1].set_title('ES True')\n",
    "            axes[0][1].axis('off')\n",
    "\n",
    "            axes[1][0].imshow(ED_Frame_IMG)\n",
    "            axes[1][0].set_title('ED Pred')\n",
    "            axes[1][0].axis('off')\n",
    "\n",
    "            axes[1][1].imshow(TrueED_Frame)\n",
    "            axes[1][1].set_title('ED True')\n",
    "            axes[1][1].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    # Crop the padding added in train\n",
    "    ES_Frame_IMG = ES_Frame_IMG.numpy()[8:-8, 8:-8, :]\n",
    "    ED_Frame_IMG = ED_Frame_IMG.numpy()[8:-8, 8:-8, :]\n",
    "\n",
    "    return ES_Frame_IMG, ED_Frame_IMG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unet Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: conv_block\n",
    "def conv_block(inputs=None, n_filters=32, dropout_prob=0, max_pooling=True):\n",
    "    \"\"\"\n",
    "    Convolutional downsampling block\n",
    "\n",
    "    Arguments:\n",
    "        inputs -- Input tensor\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        dropout_prob -- Dropout probability\n",
    "        max_pooling -- Use MaxPooling2D to reduce the spatial dimensions of the output volume\n",
    "    Returns:\n",
    "        next_layer, skip_connection --  Next layer and skip connection outputs\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    conv = Conv2D(n_filters,  # Number of filters\n",
    "                  3,  # Kernel size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(inputs)\n",
    "    conv = Conv2D(n_filters,  # Number of filters\n",
    "                  3,  # Kernel size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(conv)\n",
    "    ### END CODE HERE\n",
    "\n",
    "    # if dropout_prob > 0 add a dropout layer, with the variable dropout_prob as parameter\n",
    "    if dropout_prob > 0:\n",
    "        ### START CODE HERE\n",
    "        conv = Dropout(dropout_prob)(conv)\n",
    "        ### END CODE HERE\n",
    "\n",
    "    # if max_pooling is True add a MaxPooling2D with 2x2 pool_size\n",
    "    if max_pooling:\n",
    "        ### START CODE HERE\n",
    "        next_layer = MaxPooling2D(2, strides=2)(conv)\n",
    "        ### END CODE HERE\n",
    "\n",
    "    else:\n",
    "        next_layer = conv\n",
    "\n",
    "    skip_connection = conv\n",
    "\n",
    "    return next_layer, skip_connection\n",
    "\n",
    "\n",
    "# UNQ_C2\n",
    "# GRADED FUNCTION: upsampling_block\n",
    "def upsampling_block(expansive_input, contractive_input, n_filters=32):\n",
    "    \"\"\"\n",
    "    Convolutional upsampling block\n",
    "\n",
    "    Arguments:\n",
    "        expansive_input -- Input tensor from previous layer\n",
    "        contractive_input -- Input tensor from previous skip layer\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "    Returns:\n",
    "        conv -- Tensor output\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    up = Conv2DTranspose(\n",
    "        n_filters,  # number of filters\n",
    "        3,  # Kernel size\n",
    "        strides=2,\n",
    "        padding='same')(expansive_input)\n",
    "\n",
    "    # Merge the previous output and the contractive_input\n",
    "    merge = concatenate([up, contractive_input], axis=3)\n",
    "\n",
    "    conv = Conv2D(n_filters,  # Number of filters\n",
    "                  3,  # Kernel size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(merge)\n",
    "    conv = Conv2D(n_filters,  # Number of filters\n",
    "                  3,  # Kernel size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal')(conv)\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "# UNQ_C3\n",
    "# GRADED FUNCTION: unet_model\n",
    "def unetModel(input_size, n_filters, n_classes):\n",
    "    \"\"\"\n",
    "    Unet model\n",
    "\n",
    "    Arguments:\n",
    "        input_size -- Input shape\n",
    "        n_filters -- Number of filters for the convolutional layers\n",
    "        n_classes -- Number of output classes\n",
    "    Returns:\n",
    "        model -- tf.keras.Model\n",
    "    \"\"\"\n",
    "    inputs = Input(input_size)\n",
    "    # Contracting Path (encoding)\n",
    "    # Add a conv_block with the inputs of the unet_ model and n_filters\n",
    "    ### START CODE HERE\n",
    "    cblock1 = conv_block(inputs=inputs, n_filters=n_filters * 1)\n",
    "    # Chain the first element of the output of each block to be the input of the next conv_block.\n",
    "    # Double the number of filters at each new step\n",
    "    cblock2 = conv_block(inputs=cblock1[0], n_filters=n_filters * 2)\n",
    "    cblock3 = conv_block(inputs=cblock2[0], n_filters=n_filters * 4)\n",
    "    # Include a dropout of 0.3 for this layer\n",
    "    cblock4 = conv_block(inputs=cblock3[0], n_filters=n_filters * 8, dropout_prob=0.3)\n",
    "    # Include a dropout of 0.3 for this layer, and avoid the max_pooling layer\n",
    "    cblock5 = conv_block(inputs=cblock4[0], n_filters=n_filters * 16, dropout_prob=0.3, max_pooling=False)\n",
    "    ### END CODE HERE\n",
    "\n",
    "    # Expanding Path (decoding)\n",
    "    # Add the first upsampling_block.\n",
    "    # From here,at each step, use half the number of filters of the previous block\n",
    "    # Use the cblock5[0] as expansive_input and cblock4[1] as contractive_input and n_filters * 8\n",
    "    ### START CODE HERE\n",
    "    ublock6 = upsampling_block(cblock5[0], cblock4[1], n_filters * 8)\n",
    "    # Chain the output of the previous block as expansive_input and the corresponding contractive block output.\n",
    "    # Note that you must use the second element of the contractive block i.e before the maxpooling layer.\n",
    "\n",
    "    ublock7 = upsampling_block(ublock6, cblock3[1], n_filters * 4)\n",
    "    ublock8 = upsampling_block(ublock7, cblock2[1], n_filters * 2)\n",
    "    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters * 1)\n",
    "    ### END CODE HERE\n",
    "\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                   3,\n",
    "                   activation='relu',\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\n",
    "    ### START CODE HERE\n",
    "    conv10 = Conv2D(n_classes, 1, padding='same')(conv9)\n",
    "    ### END CODE HERE\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unet Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predictLVForEDESFrames(ES_Frame_IMG, ED_Frame_IMG, ED_model, ES_model):\n",
    "    # Mask\n",
    "    ED_pred_mask = predictMask(ED_model, np.expand_dims(ED_Frame_IMG, axis=0))\n",
    "    ES_pred_mask = predictMask(ES_model, np.expand_dims(ES_Frame_IMG, axis=0))\n",
    "\n",
    "    # Volume\n",
    "    ED_pred_volume, ED_pred_landmarks = get_LV_volume(ED_pred_mask)\n",
    "    ES_pred_volume, ES_pred_landmarks = get_LV_volume(ES_pred_mask)\n",
    "\n",
    "    # EF\n",
    "    ef_pred = calculate_EF(ED_pred_volume, ES_pred_volume)\n",
    "\n",
    "    return ef_pred\n",
    "\n",
    "\n",
    "def calculate_mean_mse(ground_truth_masks, predicted_masks):\n",
    "    num_masks = len(predicted_masks)\n",
    "    mse_values = []\n",
    "\n",
    "    for i in range(num_masks):\n",
    "        mse = mean_squared_error(ground_truth_masks[i].flatten(), predicted_masks[i].flatten())\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    mean_mse = np.mean(mse_values)\n",
    "    print(f\"Mean MSE: {mean_mse * 100}\")\n",
    "    # return mean_mse\n",
    "\n",
    "\n",
    "def _createPredictedMask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def predictMask(model, image):\n",
    "    mask = _createPredictedMask(model.predict(image))\n",
    "    mask = mask.numpy()\n",
    "    mask = np.squeeze(mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def loadUnetModel(path):\n",
    "    loaded_model = unetModel(input_size=(112, 112, 3), n_filters=32, n_classes=2)\n",
    "    loaded_model.load_weights(path)\n",
    "    loaded_model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "    # print(loaded_model.get_weights()[0][0][0][0])\n",
    "\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def trainUnet(dataSet, epochs=5, batchSize=32, modelPath='', name=''):\n",
    "    unet = unetModel(input_size=(112, 112, 3), n_filters=32, n_classes=2)\n",
    "    unet.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    # unet.summary()\n",
    "\n",
    "    BUFFER_SIZE = 500\n",
    "    dataSet.batch(batchSize)\n",
    "    dataSet = dataSet.cache().shuffle(BUFFER_SIZE).batch(batchSize)\n",
    "\n",
    "    unet.fit(dataSet, epochs=epochs)\n",
    "\n",
    "    unet.save_weights(f'{modelPath}/{name}.weights.h5')\n",
    "\n",
    "\n",
    "def testUnet(dataSet, path=''):\n",
    "    dataSet.batch(1)\n",
    "    dataSet = dataSet.cache().batch(1)\n",
    "\n",
    "    loaded_model = loadUnetModel(path)\n",
    "\n",
    "    evaluation_result = loaded_model.evaluate(dataSet)\n",
    "    print(\"Test Accuracy:\", evaluation_result[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train & Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Data\n",
    "train_dataSet = load_or_get_data(spilt_type='TRAIN')\n",
    "print('TRAIN =', len(train_dataSet))\n",
    "\n",
    "test_dataSet = load_or_get_data(spilt_type='TEST')\n",
    "print('TEST =', len(test_dataSet))\n",
    "\n",
    "val_dataSet = load_or_get_data(spilt_type='VAL')\n",
    "print('VAL =', len(val_dataSet))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "# Train Note: Use 2 GPU to get fast train\n",
    "# Transformer.train(train_dataSet, num_epochs=7, batch_size=2, parallel=True)\n",
    "# Train Note: Use CPU\n",
    "trainTransformer(train_dataSet, num_epochs=7, batch_size=1, parallel=True)\n",
    "\n",
    "testTransformer(_transformerModelPath, test_dataSet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare Data For U-NET\n",
    "CreateAllMasks(_trueMasksPath)\n",
    "\n",
    "frameType = 'ED'\n",
    "unet_dataset_train = getImageAndMasks(frameType=frameType, split='TRAIN', trueMasksPath=_trueMasksPath)\n",
    "unet_dataset_test = getImageAndMasks(frameType=frameType, split='TEST', trueMasksPath=_trueMasksPath)\n",
    "unet_dataset_val = getImageAndMasks(frameType=frameType, split='VAL', trueMasksPath=_trueMasksPath)\n",
    "\n",
    "# ED U-NET Model\n",
    "trainUnet(unet_dataset_train, epochs=5, batchSize=32, modelPath='', name=f'{frameType}_U_NET_Model')\n",
    "testUnet(unet_dataset_test, path=_ED_Model_Path)\n",
    "\n",
    "frameType = 'ES'\n",
    "unet_dataset_train = getImageAndMasks(frameType=frameType, split='TRAIN', trueMasksPath=_trueMasksPath)\n",
    "unet_dataset_test = getImageAndMasks(frameType=frameType, split='TEST', trueMasksPath=_trueMasksPath)\n",
    "unet_dataset_val = getImageAndMasks(frameType=frameType, split='VAL', trueMasksPath=_trueMasksPath)\n",
    "\n",
    "# 80,20 %\n",
    "N = 550\n",
    "unet_dataset_train = unet_dataset_train.concatenate(unet_dataset_val.take(N))\n",
    "unet_dataset_val = unet_dataset_val.skip(N)\n",
    "unet_dataset_test = unet_dataset_test.concatenate(unet_dataset_val)\n",
    "print(len(unet_dataset_train))\n",
    "print(len(unet_dataset_test))\n",
    "\n",
    "# ES U-NET Model\n",
    "trainUnet(unet_dataset_train, epochs=5, batchSize=32, modelPath='', name=f'{frameType}_U_NET_Model')\n",
    "testUnet(unet_dataset_test, path=_ES_Model_Path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data\n",
    "test_dataSet = load_or_get_data('TEST')\n",
    "print('TEST =', len(test_dataSet))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Model\n",
    "transformerModel = loadTransformerModel(_transformerModelPath)\n",
    "ED_Model = loadUnetModel(_ED_Model_Path)\n",
    "ES_Model = loadUnetModel(_ES_Model_Path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for obj in test_dataSet[0:1]:\n",
    "    print(obj.fileName)\n",
    "    name = obj.fileName + '.avi'\n",
    "    videoPath = os.path.join(_videosPath, name)\n",
    "\n",
    "    ES_Frame_IMG, ED_Frame_IMG = Detect_ESED_Frame(videoPath, transformerModel)\n",
    "\n",
    "    efPred = predictLVForEDESFrames(ES_Frame_IMG, ED_Frame_IMG, ED_Model, ES_Model)\n",
    "\n",
    "    print('EF Predicted =', efPred)\n",
    "    print('EF =', obj.EF_value)\n",
    "    print('-----------------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
